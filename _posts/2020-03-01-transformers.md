---
layout: post
title: ä»å¤´å¼€å§‹å®ç°ä¸€ä¸ªtransformer
description: ä»å¤´å¼€å§‹å®ç°ä¸€ä¸ªtransformer

---

### å¯¼å…¥

2017å¹´ï¼ŒGoogle åœ¨è®ºæ–‡ã€Š[Attention Is All You Need](https://arxiv.org/abs/1706.03762)ã€‹ä¸­æå‡º Transformer,**å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒå¾ªç¯ä¸å·ç§¯ï¼Œå®ç°å¹¶è¡ŒåŒ–è®­ç»ƒï¼Œå»ºæ¨¡é•¿è·ç¦»ä¾èµ–**ï¼Œæ ¸å¿ƒä¼˜åŠ¿ï¼š

| ç‰¹æ€§        | è¯´æ˜                         |
| --------- | -------------------------- |
| **å¹¶è¡Œè®¡ç®—**  | ä¸åƒ RNN éœ€é¡ºåºè®¡ç®—ï¼Œå¯å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—     |
| **é•¿è·ç¦»ä¾èµ–** | æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å»ºæ¨¡ä»»æ„ä½ç½®é—´å…³ç³»           |
| **å¯æ‰©å±•æ€§å¼º** | æ˜“å †å ã€æ˜“æ‰©å±•åˆ°æ›´å¤§æ¨¡å‹               |
| **ç»Ÿä¸€æ¶æ„**  | å¯ç”¨äºç¿»è¯‘ã€æ‘˜è¦ã€åˆ†ç±»ã€ç”Ÿæˆç­‰å‡ ä¹æ‰€æœ‰ NLP ä»»åŠ¡ |

å…¶ä¸­æœ€ä¸»è¦çš„ä¸€ç‚¹å°±æ˜¯æ˜“å †å ï¼Œä½ æƒ³è±¡ä¸€ä¸‹ï¼Œå‡ åä¸Šç™¾å±‚çš„Transformerå±‚å †å åœ¨ä¸€èµ·ï¼Œé…åˆå¤§è§„æ¨¡é›†ç¾¤å’Œå¤§é‡æ–‡æœ¬æ•°æ®é›†çš„è®­ç»ƒï¼Œé‚£æ•ˆæœå¾ˆéš¾æƒ³è±¡ã€‚è¿˜æœ‰å¹¶è¡Œè®¡ç®—ï¼Œä¹Ÿæ­£å¥½æ˜¯ä¸ºäº†å‰è€…çš„å¤§è§„æ¨¡è®­ç»ƒåšé“ºå«ã€‚



**å…¶å¤§æ¦‚çš„ç»“æ„å¦‚ä¸‹**:

```
      è¾“å…¥åºåˆ—
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Embedding  â”‚ â† è¯åµŒå…¥ + ä½ç½®ç¼–ç 
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Encoder   â”‚ â† N ä¸ªç›¸åŒå±‚å †å 
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Decoder   â”‚ â† N ä¸ªå¸¦æ©ç çš„å±‚å †å 
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Linear +   â”‚
    â”‚  Softmax    â”‚ â† è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è®ºæ–‡ä¸­çš„å›¾æ›´åŠ æ¸…æ™°ï¼šï¼ˆåŸè®ºæ–‡å›¾æ˜¯åˆ†å¼€çš„ï¼‰**

![](https://github.com/cryer/cryer.github.io/raw/master/image/161.jpg)

ç†Ÿæ‚‰RNNçš„è¯»è€…åº”è¯¥çŸ¥é“ï¼Œè¿™ç§`encoder-decoder`ç»“æ„æœ€æ“…é•¿çš„å°±æ˜¯`seq2seq`ï¼Œåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ¯”å¦‚æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ã€‚ä½†ä½ çŸ¥é“å…¶å®åªæœ‰`decoder`çš„ç»“æ„ä¹Ÿæ“…é•¿ä¸€ä¸ªé¢†åŸŸå—ï¼Ÿé‚£å°±æ˜¯ç”Ÿæˆï¼Œåªæœ‰`decoder`å°±ç›¸å½“äºä¸€ä¸ª**è‡ªå›å½’æ¨¡å‹**ï¼Œé€šè¿‡èµ·å§‹tokené¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶ååŠ å…¥åˆ°è¾“å…¥tokenåºåˆ—ä¸­ä½œä¸ºæ¡ä»¶ï¼Œç»§ç»­é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œå°±æ˜¯æœ€åŸºæœ¬çš„ç”Ÿæˆä»»åŠ¡çš„è‡ªå›å½’æ¨¡å‹æ¶æ„ã€‚

æ‰€ä»¥è¿™ç¯‡åšå®¢ï¼Œä¸»è¦å°±å®ç°ä¸€ä¸ªåªæœ‰`decoder`çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

### æ­£æ–‡

**é¡¹ç›®ç»“æ„ï¼š**

```
decoder_only
â”œâ”€data 
â”‚â€ƒâ””â”€data.txt 
â”œâ”€model.py 
â”œâ”€inference.py 
â””â”€train.py 
â””â”€make_dataset.py 
```

#### æ•°æ®é›†åˆ¶ä½œï¼š

```python
# make_dataset.py
import torch
from torch.utils.data import Dataset

class TextDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_len=128):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.data = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        tokens = tokenizer.encode(text, add_special_tokens=False)
        
        for i in range(0, len(tokens) - max_len, max_len // 2):
            input_ids = tokens[i:i + max_len - 1]
            target_ids = tokens[i + 1:i + max_len]
            
            input_ids += [PAD_TOKEN] * (max_len - 1 - len(input_ids))
            target_ids += [PAD_TOKEN] * (max_len - 1 - len(target_ids))
            
            self.data.append({
                'input_ids': torch.tensor(input_ids, dtype=torch.long),
                'target_ids': torch.tensor(target_ids, dtype=torch.long)
            })
        
        print(f"å…±æ„å»º {len(self.data)} ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]['input_ids'], self.data[idx]['target_ids']
```

#### æ¨¡å‹


```python
# model.py
import torch
import torch.nn as nn
import math

class SinusoidalPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(SinusoidalPositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x


class MultiheadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=False):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.batch_first = batch_first
        
        assert embed_dim % num_heads == 0, 
        self.head_dim = embed_dim // num_heads
        
        # æ€»å…±4ä¸ªçº¿æ€§å±‚ï¼šQ, K, V, è¾“å‡ºæŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout_layer = nn.Dropout(dropout)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, query, key, value, attn_mask=None):
        """
        è¾“å…¥:(if batch_first=True)
            query: [batch_size, seq_len, embed_dim] 
            key:   [batch_size, seq_len, embed_dim]
            value: [batch_size, seq_len, embed_dim]
            attn_mask: [seq_len, seq_len] æˆ– [batch_size, seq_len, seq_len] (å¯é€‰ï¼Œboolæˆ–float)
        
        è¿”å›:
            output: [batch_size, seq_len, embed_dim]
            attn_weights: [batch_size, num_heads, seq_len, seq_len]
        """
        if not self.batch_first:
            # å¦‚æœä¸æ˜¯ batch_firstï¼Œè½¬ç½®ä¸º [seq_len, batch_size, embed_dim]
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)
        
        batch_size, seq_len, embed_dim = query.size()
        
        # çº¿æ€§å˜æ¢
        Q = self.q_proj(query)  # [B, L, E]
        K = self.k_proj(key)    # [B, L, E]
        V = self.v_proj(value)  # [B, L, E]
        
        # æ‹†åˆ†å¤šå¤´: [B, L, E] -> [B, L, H, D] -> [B, H, L, D]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›: [B, H, L, D] @ [B, H, D, L] -> [B, H, L, L]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚å› æœmaskï¼‰
        if attn_mask is not None:
            if attn_mask.dim() == 2:
                # [L, L] -> [1, 1, L, L] å¹¿æ’­åˆ°æ‰€æœ‰batchå’Œhead
                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)
            elif attn_mask.dim() == 3:
                # [B, L, L] -> [B, 1, L, L]
                attn_mask = attn_mask.unsqueeze(1)
            # å°† mask ä¸­ True çš„ä½ç½®ï¼ˆè¢«å±è”½ï¼‰è®¾ä¸º -inf
            # æ ¹æ® scores çš„ dtype é€‰æ‹©åˆé€‚çš„ mask å€¼
            # è‡ªåŠ¨è·å–å½“å‰æ•°æ®ç±»å‹çš„æœ€å°å€¼
            mask_value = torch.finfo(scores.dtype).min  
            scores = scores.masked_fill(attn_mask, mask_value)
        
        # softmax + dropout
        attn_weights = self.softmax(scores)
        attn_weights = self.dropout_layer(attn_weights)
        
        # åŠ æƒæ±‚å’Œ: [B, H, L, L] @ [B, H, L, D] -> [B, H, L, D]
        output = torch.matmul(attn_weights, V)
        
        # åˆå¹¶å¤šå¤´: [B, H, L, D] -> [B, L, H, D] -> [B, L, E]
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.out_proj(output)
        
        if not self.batch_first:
            # è½¬å› [seq_len, batch_size, embed_dim]
            output = output.transpose(0, 1)
            attn_weights = attn_weights.transpose(0, 1)  ]ï¼Œ
        
        return output, attn_weights


class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):
        super(TransformerDecoderLayer, self).__init__()
        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.activation = nn.ReLU()

    # tgt: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, d_model]
    # tgt_mask: æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ [seq_len, seq_len]æˆ–[batch_size, seq_len, seq_len] ï¼ˆé€šå¸¸æ˜¯ä¸‹ä¸‰è§’æ©ç ï¼‰
    def forward(self, tgt, tgt_mask=None):
        # å–ç¬¬ä¸€è¿”å›å€¼ï¼ˆoutputï¼‰
        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]
        # æ®‹å·®è¿æ¥ ç»´åº¦ä¸€ç›´æ˜¯[B, L, E]å³[batch_size, seq_len, d_model]
        # æ®‹å·®è¿æ¥å‰å¯¹æ³¨æ„åŠ›è¾“å‡ºåš dropoutï¼ˆè®ºæ–‡æ¨èï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        tgt = tgt + self.dropout1(tgt2)
        # LayerNormï¼šæ¯ä¸ªtokençš„d_modelç»´åº¦åšå½’ä¸€åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
        # [batch_size, seq_len, d_model]
        tgt = self.norm1(tgt)
        # å‰é¦ˆç¥ç»ç½‘è·¯Linear â†’ ReLU â†’ Dropout â†’ Linear
        # linear1ï¼šd_model â†’ dim_feedforwardï¼Œå¦‚ 256 â†’ 512 æŠ•å½±åˆ°é«˜ç»´
        # linear1 ï¼šdim_feedforward â†’ d_model å›åˆ°åŸç»´åº¦[B, L, E] 
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        # æ®‹å·®è¿æ¥
        tgt = tgt + self.dropout2(tgt2)
        # LayerNorm [B, L, E]
        tgt = self.norm2(tgt)
        return tgt

class TransformerDecoderLM(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1, max_len=128):
        super(TransformerDecoderLM, self).__init__()
        self.d_model = d_model
        self.max_len = max_len
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)
        self.dropout = nn.Dropout(dropout)
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])
        self.fc_out = nn.Linear(d_model, vocab_size)
        self.init_weights()
    
    def init_weights(self):
        initrange = 0.1
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc_out.bias.data.zero_()
        self.fc_out.weight.data.uniform_(-initrange, initrange)

    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()
        return mask.to(DEVICE)

    def forward(self, src):
        # src:[batch_size, seq_len]
        # è·å–è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼ˆtoken æ•°é‡ï¼‰ä¹Ÿå°±æ˜¯seq_len
        # è‡ªå›å½’æ©ç ï¼ˆcausal maskï¼‰ éœ€è¦çŸ¥é“åºåˆ—é•¿åº¦
        # ä½ç½®ç¼–ç ä¹Ÿå¯èƒ½ä¾èµ–é•¿åº¦ï¼ˆè™½ç„¶æˆ‘ä»¬çš„ pos_encoder æ”¯æŒä»»æ„é•¿åº¦ï¼‰
        seq_len = src.size(1)
        # token ID æ˜ å°„ä¸ºç¨ å¯†å‘é‡ï¼ˆè¯åµŒå…¥ï¼‰
        # ç„¶åä¹˜ä»¥ âˆšd_model è¿›è¡Œç¼©æ”¾ 
        #  [B, L] -> [B, L, E]
        src = self.embedding(src) * math.sqrt(self.d_model)
        # ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
        # [B, L, E] -> [B, L, E]
        src = self.pos_encoder(src)
        # å¯¹è¯åµŒå…¥ + ä½ç½®ç¼–ç åçš„å‘é‡åº”ç”¨ Dropoutï¼ˆéšæœºç½®é›¶ä¸€éƒ¨åˆ†å…ƒç´ ï¼‰
        # é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
        src = self.dropout(src)
        # ç”Ÿæˆâ€œä¸‹ä¸‰è§’æ©ç â€ï¼Œç”¨äºè‡ªå›å½’ç”Ÿæˆï¼Œé˜²æ­¢å½“å‰ä½ç½®çœ‹åˆ°æœªæ¥ token
        tgt_mask = self.generate_square_subsequent_mask(seq_len)
        # é€å±‚é€šè¿‡ Transformer Decoder å±‚
        # æ¯ä¸€å±‚éƒ½è¿›è¡Œï¼šè‡ªæ³¨æ„åŠ›ï¼ˆå¸¦mask)+å‰é¦ˆç½‘ç»œ+æ®‹å·®è¿æ¥+LayerNorm
        for layer in self.decoder_layers:
            src = layer(src, tgt_mask=tgt_mask)
        # æ¯ä¸ªä½ç½®çš„éšè—çŠ¶æ€æ˜ å°„åˆ°è¯è¡¨ç©ºé—´ï¼Œå¾—åˆ° logits
        # [B, L, E] -> [B, L, V]
        output = self.fc_out(src)
        return output
```

åŸºæœ¬ç…§ç€å¯¼å…¥ä¸­çš„å›¾å°±èƒ½ç›´æ¥ç¼–å†™ï¼Œéå¸¸ç®€å•ï¼Œæœ¬ä¾‹å°±æ˜¯4ä¸ª`decoder`å±‚çš„å †å ï¼Œå¤šå¤´æ³¨æ„åŠ›å±‚åˆ†8ä¸ªå¤´ï¼Œä½¿ç”¨`sum(p.numel() for p in model.parameters())`æ¥æ‰“å°ä¸€ä¸‹å‚æ•°æ€»æ•°ï¼Œç»“æœæ˜¯`12,947,080`ï¼Œä¹Ÿå°±æ˜¯12Mçš„å‚æ•°é‡ï¼Œä¹Ÿå°±ç›¸å½“äº`resnet-50`çš„ä¸€åŠï¼Œå¯ä»¥è¯´å¾ˆå°ã€‚è¿˜éœ€è¦æ³¨æ„è¿™é‡Œçš„ä¸€ä¸ªæŠ€å·§ï¼Œå°±æ˜¯æ³¨æ„åŠ›å±‚åˆ†å¤´é‚£é‡Œï¼Œé‡‡ç”¨çš„æ˜¯ç›´æ¥ä¸‰ç»´åˆ†åˆ°å››ç»´å¼ é‡ï¼Œè€Œä¸æ˜¯å•ç‹¬ç”¨å˜é‡åˆ†å¼€å­˜å‚¨åˆ†å¤´çš„ä¸‰ç»´å¼ é‡ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥å……åˆ†é«˜æ•ˆåˆ©ç”¨å¼ é‡çš„å¹¶è¿ç®—ã€‚åŒæ—¶æ³¨æ„åˆ°ç»´åº¦2ï¼Œ3ç»´äº’æ¢äº†ï¼Œå°†å¤´æ•°æ”¾åˆ°å‰é¢ï¼Œè¿™æ˜¯ä¸ºäº†åé¢çš„çŸ©é˜µä¹˜æ³•å¤„ç†çš„æ˜¯æ¯ä¸ªå¤´å†…éƒ¨çš„è®¡ç®—ã€‚

å¦å¤–æ³¨æ„åˆ°æˆ‘ä»¬å¹¶æ²¡æœ‰å®ç°å®Œæ•´çš„`decoder`ï¼Œæˆ‘ä»¬åªå †å äº†`Masked Multi-Head Attention`è¿™ä¸ªå±‚ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨`Multi-Head Attention`,è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ˜¯è‡ªå›å½’ç”Ÿæˆä»»åŠ¡ï¼Œåªéœ€è¦å¤„ç†`self attention`è‡ªæ³¨æ„åŠ›ï¼Œè€Œ`Multi-Head Attention`å…¶å®æ˜¯`cross attention`äº¤å‰æ³¨æ„åŠ›ï¼Œéœ€è¦ä¾èµ–`encoder`æä¾›çš„**KV**å‘é‡ï¼Œè€Œæˆ‘ä»¬ä¸éœ€è¦ã€‚

æ•´ä½“å°±æ˜¯è¿™æ ·ï¼š

```

è¾“å…¥ tgt â”€â”€â”€â”
            â”‚
            â–¼
     [Multihead Self-Attention]  â†â”€ attn_maskï¼ˆå› æœæ©ç ï¼‰
            â”‚
            â–¼
        [Dropout] â”€â”€â”€â”
            â”‚        â”‚
            â–¼        â”‚
   [Add: tgt + tgt2] â”‚ â†â”€â”€ æ®‹å·®è¿æ¥
            â”‚        â”‚
            â–¼        â”‚
      [LayerNorm]    â”‚
            â”‚        â”‚
            â–¼        â”‚
[Linear â†’ ReLU â†’ Dropout â†’ Linear]  â†â”€ FFN
            â”‚
            â–¼
        [Dropout] â”€â”€â”€â”
            â”‚        â”‚
            â–¼        â”‚
   [Add: tgt + tgt2] â”‚ â†â”€â”€ æ®‹å·®è¿æ¥
            â”‚        â”‚
            â–¼        â”‚
      [LayerNorm] â”€â”€â”€â”˜
            â”‚
            â–¼
         è¾“å‡º tgt
```

#### è®­ç»ƒ

```python
# train.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import BertTokenizer
from tqdm import tqdm
# æ··åˆç²¾åº¦æ”¯æŒ
from torch.cuda.amp import GradScaler, autocast

from model import TransformerDecoderLM
from make_dataset import TextDataset

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

D_MODEL = 256
NHEAD = 8
NUM_LAYERS = 4
DIM_FF = 512
DROPOUT = 0.1
MAX_LEN = 256
BATCH_SIZE = 32
LEARNING_RATE = 3e-4
EPOCHS = 10
PAD_TOKEN = 0

DATA_PATH = "./data/data.txt"
MODEL_SAVE_PATH = "./decoder_only.pth"

def train_model(model, dataloader, epochs=10, lr=3e-4):
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)
    scaler = GradScaler()  # æ··åˆç²¾åº¦ç¼©æ”¾å™¨

    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch_idx, (input_ids, target_ids) in enumerate(progress_bar):
            input_ids = input_ids.to(DEVICE)
            target_ids = target_ids.to(DEVICE)
            
            optimizer.zero_grad()
            
            with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦
                output = model(input_ids)
                output = output.view(-1, output.size(-1))
                target = target_ids.view(-1)
                loss = criterion(output, target)
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item()})
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹å’Œé…ç½®ï¼Œå¯ä»¥æŠŠä¸€äº›å‚æ•°è®¾ç½®éƒ½ä¿å­˜ä¸€ä¸‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': {
            'd_model': D_MODEL,
            'nhead': NHEAD,
            'num_layers': NUM_LAYERS,
            'vocab_size': tokenizer.vocab_size if 'tokenizer' in globals() else 21128,
            'max_len': MAX_LEN
        },
        'sampling_config': {
            'temperature': 0.7,
            'top_k': 40,
            'top_p': 0.9
        }
    }, MODEL_SAVE_PATH)
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_SAVE_PATH}")



def main():
    print("æ­£åœ¨åŠ è½½ä¸­æ–‡åˆ†è¯å™¨...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    vocab_size = tokenizer.vocab_size
    print(f"è¯è¡¨å¤§å°: {vocab_size}")
    
    if not os.path.exists(DATA_PATH):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ {DATA_PATH}")
        print(f"è¯·åœ¨ ./data/ ç›®å½•ä¸‹æ”¾ç½® {DATA_PATH} æ–‡ä»¶")
        return
    
    print("æ­£åœ¨æ„å»ºæ•°æ®é›†...")
    dataset = TextDataset(DATA_PATH, tokenizer, max_len=MAX_LEN)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    model = TransformerDecoderLM(
        vocab_size=vocab_size,
        d_model=D_MODEL,
        nhead=NHEAD,
        num_layers=NUM_LAYERS,
        dim_feedforward=DIM_FF,
        dropout=DROPOUT,
        max_len=MAX_LEN
    ).to(DEVICE)
    
    print("å¼€å§‹è®­ç»ƒï¼ˆæ··åˆç²¾åº¦ï¼‰...")
    train_model(model, dataloader, epochs=EPOCHS, lr=LEARNING_RATE)
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    
if __name__ == "__main__":
    main()
```



### æ¨ç†

```python
import torch
import torch.nn as nn
import math
from transformers import BertTokenizer
import os 
from model import TransformerDecoderLM

# ========================
# é…ç½®å‚æ•°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# ========================
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"æ¨ç†è®¾å¤‡: {DEVICE}")

# æ¨¡å‹ç»“æ„å‚æ•°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
D_MODEL = 256
NHEAD = 8
NUM_LAYERS = 4
DIM_FF = 512
DROPOUT = 0.1
MAX_LEN = 256  # æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦

# æ¨¡å‹è·¯å¾„
MODEL_SAVE_PATH = "./decoder_only.pth"

# é»˜è®¤é‡‡æ ·å‚æ•°ï¼ˆå¯è¿è¡Œæ—¶è°ƒæ•´ï¼‰
DEFAULT_TEMPERATURE = 0.7
DEFAULT_TOP_K = 40
DEFAULT_TOP_P = 0.9 


def generate_text(
    model,
    tokenizer,
    prompt,
    max_new_tokens=50,  
    temperature=0.7,
    top_k=40,
    top_p=0.9,
    stop_tokens=None        # å¯é€‰ï¼šè‡ªå®šä¹‰åœæ­¢token
):
    """
    æ ¹æ®æç¤ºè¯ç”Ÿæˆæ–‡æœ¬ï¼Œæ”¯æŒæ§åˆ¶ç”Ÿæˆé•¿åº¦å’Œé‡‡æ ·ç­–ç•¥
    å‚æ•°:
        max_new_tokens: ç”Ÿæˆçš„æ–°tokenæ•°é‡ï¼ˆä¸åŒ…æ‹¬è¾“å…¥ï¼‰
        temperature: æ¸©åº¦å€¼ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆå€¼è¶Šå°è¶Šç¡®å®šï¼‰
        top_k: ä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯
        top_p: nucleusé‡‡æ ·é˜ˆå€¼
        stop_tokens: é‡åˆ°è¿™äº›tokenæ—¶æå‰åœæ­¢ï¼ˆå¦‚å¥å·ã€é—®å·ç­‰ï¼‰
    """
    model.eval()
    
    if stop_tokens is None:
        # é»˜è®¤é‡åˆ°è¿™äº›ç‰¹æ®Štokenåœæ­¢
        stop_tokens = set([0, 102, 103])  # PAD, SEP, UNK
        # å¯æ·»åŠ ä¸­æ–‡å¥å·ã€é—®å·ç­‰
        period_id = tokenizer.convert_tokens_to_ids('ã€‚')
        question_id = tokenizer.convert_tokens_to_ids('ï¼Ÿ')
        exclamation_id = tokenizer.convert_tokens_to_ids('ï¼')
        if period_id != tokenizer.unk_token_id:
            stop_tokens.add(period_id)
        if question_id != tokenizer.unk_token_id:
            stop_tokens.add(question_id)
        if exclamation_id != tokenizer.unk_token_id:
            stop_tokens.add(exclamation_id)
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(DEVICE)
    generated = input_ids.clone()
    
    with torch.no_grad():
        for i in range(max_new_tokens):  # æ§åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦
            current_len = generated.size(1)
            if current_len >= MAX_LEN:   # è¶…è¿‡æ¨¡å‹æœ€å¤§æ”¯æŒé•¿åº¦
                print("âš ï¸  è­¦å‘Šï¼šè¾¾åˆ°æ¨¡å‹æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œåœæ­¢ç”Ÿæˆ")
                break
            
            # è·å–æ¨¡å‹è¾“å‡º
            output = model(generated)  # [1, seq_len, vocab_size]
            next_token_logits = output[:, -1, :] / temperature  # å–æœ€åä¸€ä¸ªä½ç½®
            
            # Top-k è¿‡æ»¤
            if top_k > 0:
                top_k_vals, _ = torch.topk(next_token_logits, top_k)
                kth_val = top_k_vals[:, -1].unsqueeze(-1)
                indices_to_remove = next_token_logits < kth_val
                next_token_logits[indices_to_remove] = -float('inf')
            
            # Top-p è¿‡æ»¤
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_token_logits[indices_to_remove] = -float('inf')
            
            # é‡‡æ ·
            if (next_token_logits == -float('inf')).all():
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            else:
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°åœæ­¢token
            if next_token.item() in stop_tokens:
                # å¦‚æœæ˜¯å¥å·ç±»ï¼Œå¯ä»¥ç”Ÿæˆååœæ­¢ï¼›å¦‚æœæ˜¯PADåˆ™ç›´æ¥åœæ­¢
                if next_token.item() in [0, 102, 103]:
                    break
                else:
                    # å¦‚æœæ˜¯å¥å·ï¼Œç”Ÿæˆåå†åœæ­¢
                    generated = torch.cat([generated, next_token], dim=1)
                    break
            
            generated = torch.cat([generated, next_token], dim=1)
    
    # è§£ç ä¸ºæ–‡æœ¬
    generated_text = tokenizer.decode(generated.squeeze().tolist(), skip_special_tokens=True)
    return generated_text


# ========================
# ä¸»æ¨ç†å‡½æ•°ï¼ˆäº¤äº’å¼ï¼‰
# ========================

def main():
    print("æ­£åœ¨åŠ è½½ä¸­æ–‡åˆ†è¯å™¨...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    
    # ä»ä¿å­˜æ–‡ä»¶ä¸­è¯»å–æ¨¡å‹é…ç½®
    if os.path.exists(MODEL_SAVE_PATH):
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)
        
        # å°è¯•ä»checkpointè·å–vocab_sizeï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤
        vocab_size = checkpoint.get('config', {}).get('vocab_size', tokenizer.vocab_size)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = TransformerDecoderLM(
            vocab_size=vocab_size,
            d_model=D_MODEL,
            nhead=NHEAD,
            num_layers=NUM_LAYERS,
            dim_feedforward=DIM_FF,
            dropout=DROPOUT,
            max_len=MAX_LEN
        ).to(DEVICE)
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # è¯»å–é»˜è®¤é‡‡æ ·å‚æ•°
        sampling_config = checkpoint.get('sampling_config', {})
        temperature = sampling_config.get('temperature', DEFAULT_TEMPERATURE)
        top_k = sampling_config.get('top_k', DEFAULT_TOP_K)
        top_p = sampling_config.get('top_p', DEFAULT_TOP_P)
        
    else:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_SAVE_PATH}")
        return
    
    print("\n" + "="*60)
    print("ğŸ‰ ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆæ¨ç†ç³»ç»Ÿå·²å¯åŠ¨ï¼")
    print("è¾“å…¥ä¸­æ–‡æç¤ºè¯ï¼Œæ¨¡å‹å°†è‡ªåŠ¨ç»­å†™ã€‚è¾“å…¥ 'quit' é€€å‡ºã€‚")
    print(f"é»˜è®¤å‚æ•°: temperature={temperature}, top_k={top_k}, top_p={top_p}")
    print("="*60)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            prompt = input("\nğŸ“ è¯·è¾“å…¥æç¤ºè¯ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰: ").strip()
            if prompt.lower() == 'quit':
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not prompt:
                print("âŒ æç¤ºè¯ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                continue
            
            # è¯¢é—®ç”Ÿæˆé•¿åº¦
            try:
                max_len_input = input("ğŸ“ è¯·è¾“å…¥å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤50ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: ").strip()
                max_new_tokens = int(max_len_input) if max_len_input else 50
                if max_new_tokens < 1:
                    max_new_tokens = 50
            except ValueError:
                max_new_tokens = 50
                print("âš ï¸  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 50")
            
            # ç”Ÿæˆæ–‡æœ¬
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆ...")
            generated_text = generate_text(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_new_tokens=max_new_tokens, 
                temperature=temperature,
                top_k=top_k,
                top_p=top_p
            )
            
            print(f"\nâœ… ç”Ÿæˆç»“æœ:")
            print("-" * 40)
            print(f"è¾“å…¥: {prompt}")
            print(f"è¾“å‡º: {generated_text}")
            print("-" * 40)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡ºã€‚")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue


if __name__ == "__main__":
    main()
```

äº¤äº’å¼çš„æ¨ç†ï¼Œä½¿ç”¨æ¸©åº¦å’Œ`top_k top_p`æ¥æ§åˆ¶è¾“å‡ºé‡‡æ ·ï¼Œ`top_k`è¡¨ç¤ºå–æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯æ¥è¿›è¡Œé‡‡æ ·ï¼Œ`top_p`è¡¨ç¤ºé€‰æ‹©ç´¯ç§¯æ¦‚ç‡å¤§äºç­‰äºpçš„å‰å‡ ä¸ªè¯æ¥é‡‡æ ·ï¼Œæ¯”å¦‚æ¦‚ç‡æœ€é«˜çš„å››ä¸ªtokenåˆ†åˆ«æ˜¯0.3ï¼Œ0.2ï¼Œ0.14ï¼Œ0.11, é‚£ä¹ˆ`top_p=0.6`å°±è¡¨ç¤ºåªé€‰å‰ä¸‰ä¸ªtokené‡‡æ ·ï¼Œå› ä¸ºä»–ä»¬çš„ç´¯è®¡æ¦‚ç‡æ˜¯0.64ï¼Œå¤§äº0.6äº†ã€‚å¯ä»¥é€‰æ‹©è¾“å‡ºçš„tokené•¿åº¦ï¼Œæœ€å¤§256ï¼Œè¿™æ˜¯ç”±è®­ç»ƒæ—¶å€™çš„å‚æ•°å†³å®šçš„ï¼Œé‡åˆ°`ã€‚?!`ä¼šç›´æ¥åœæ­¢ç”Ÿæˆï¼Œè¯»è€…å¯ä»¥è‡ªè¡Œè®¾ç½®ã€‚

### æ•ˆæœ

è¿™é‡Œæˆ‘ä½¿ç”¨çš„æ˜¯ä¸€éƒ¨ç½‘ç»œå°è¯´æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¤§æ¦‚`6MB`çš„æ–—ç ´è‹ç©¹å°è¯´å‰600ç« ï¼Œåˆ†è¯å™¨ä½¿ç”¨`transformers`åº“ä¸­çš„`bert-base-chinese`ä¸­æ–‡åˆ†è¯å™¨ï¼Œè¯è¡¨å¤§å°æ˜¯21128ï¼Œä½¿ç”¨æ•°æ®é›†å¤„ç†æ¨¡å—å¤„ç†è¿‡ä¹‹åï¼Œ ä¸€å…±æ˜¯15432 ä¸ªè®­ç»ƒæ ·æœ¬ã€‚ä½¿ç”¨æ‰¹å¤§å°16ï¼Œæœ€å¤§é•¿åº¦256ï¼Œè®­ç»ƒ10ä¸ªepochï¼Œæ˜¾å­˜å ç”¨3.5GBå·¦å³ï¼Œä¸å¤§ï¼Œè®­ç»ƒæ—¶é—´ä¹Ÿä¸é•¿ï¼ŒåŠå°æ—¶ä¸åˆ°ã€‚è¯»è€…ä»¬å¯ä»¥æ ¹æ®æ˜¾å­˜è‡ªç”±çš„ä¿®æ”¹è¿™äº›å‚æ•°ï¼Œæœ€é‡è¦çš„å½“ç„¶è¿˜æœ‰å¤šå¤´æ³¨æ„åŠ›å±‚çš„å †å çš„å±‚æ•°ï¼Œè¿˜æœ‰è¯å‘é‡ç»´åº¦ï¼Œä¹Ÿå¯ä»¥ä¼˜å…ˆå¢åŠ ã€‚

```
æ¨ç†è®¾å¤‡: cuda
æ­£åœ¨åŠ è½½ä¸­æ–‡åˆ†è¯å™¨...
æ­£åœ¨åŠ è½½æ¨¡å‹...
âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼

============================================================
ğŸ‰ ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆæ¨ç†ç³»ç»Ÿå·²å¯åŠ¨ï¼
è¾“å…¥ä¸­æ–‡æç¤ºè¯ï¼Œæ¨¡å‹å°†è‡ªåŠ¨ç»­å†™ã€‚è¾“å…¥ 'quit' é€€å‡ºã€‚
é»˜è®¤å‚æ•°: temperature=0.7, top_k=40, top_p=0.9
============================================================

ğŸ“ è¯·è¾“å…¥æç¤ºè¯ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰: è§ç‚
ğŸ“ è¯·è¾“å…¥å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤50ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: 200
ğŸ¤– æ­£åœ¨ç”Ÿæˆ...

âœ… ç”Ÿæˆç»“æœ:
----------------------------------------
è¾“å…¥: è§ç‚
è¾“å‡º: è§ ç‚ çš„ ç›® å…‰ ï¼Œ ç¼“ ç¼“ çš„ åœ¨ å‘¨ å›´ æ‰« äº† æ‰« ï¼Œ å°† è¿‘ å‡  ä¸ª å° æ—¶ ï¼Œ 
è§ ç‚ è„š æ­¥ é¡¿ åœ¨ åŠ ç©º  ä¹‹ ä¸Š ï¼Œ èº« ä½“ å‡Œ ç©º ç¿» æ»š ï¼Œ åŒ æ‰‹ ç´§ ç´§ æ¡ ï¼Œ
 ä¸€ è‚¡ åŠ² æ°” è‡ª è¢– é—´ çŒ› ç„¶ æš´ æ¶Œ è€Œ å‡º ï¼Œ ç„¶ å ç‹   ç‹  åœ° ç ¸ åœ¨ äº† è§ ç‚
 çš„ èƒ¸ è†› ä¹‹ ä¸Š ã€‚
----------------------------------------

ğŸ“ è¯·è¾“å…¥æç¤ºè¯ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰: ä½ è‹¥æ•¢ä¼¤ä»–ä¸€æ ¹æ¯«æ¯›
ğŸ“ è¯·è¾“å…¥å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤50ï¼Œç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: 200
ğŸ¤– æ­£åœ¨ç”Ÿæˆ...

âœ… ç”Ÿæˆç»“æœ:
----------------------------------------
è¾“å…¥: ä½ è‹¥æ•¢ä¼¤ä»–ä¸€æ ¹æ¯«æ¯›
è¾“å‡º: ä½  è‹¥ æ•¢ ä¼¤ ä»– ä¸€ æ ¹ æ¯« æ¯› éª¨ å¤´ ï¼Œ ä½  çš„ å® åŠ› ï¼Œ ä¹Ÿ æ˜¯ æ²¡ æœ‰ é‚£ ç§
 è®© äºº å¿ƒ ä¸­ çš„ æ„Ÿ è§‰ ï¼Œ  å½“ ç„¶ ï¼Œ è§ ç‚ ä¹Ÿ ä¸ æ•¢ å† ç† ä¼š è¿™ è¯ ï¼Œ ä»– ä¾¿ æ˜¯
 ç§ è§ ä¸€ å æ–— çš‡ å¼º è€… çš„ å® åŠ› ï¼Œ è™½ ç„¶ æ–— çš‡  å¼º è€… ï¼Œ å¯ è¿™ å¯¹ ä»˜ æ•– 
è¿™ ä½ å¹´ è½» äºº çš„ å® åŠ› ï¼Œ ä¹Ÿ æ˜¯ æ ä¸º ä¸¥ å‰ ã€‚
----------------------------------------
```

ç®€å•çš„æ•ˆæœæ¼”ç¤ºï¼Œå¯¹äºæ¨¡å‹å‚æ•°ä¸å¤§ï¼Œè®­ç»ƒæ—¶é—´ä¹ŸçŸ­çš„ç¨‹åºæ¥è¯´ï¼Œæ•ˆæœè¿˜æ˜¯ä¸é”™çš„ï¼Œè‡³å°‘è¯´çš„è¯æ˜¯å¤§ä½“é€šé¡ºï¼Œèƒ½å¤Ÿé˜…è¯»çš„ã€‚è¯»è€…å¯ä»¥è‡ªå·±é€‰æ‹©æ›´å¤§çš„æ•°æ®é›†å’Œè®­ç»ƒæ›´å¤šçš„è½®æ¬¡ï¼Œå½“ç„¶è¿˜æœ‰è®¾ç½®æ›´æ·±çš„æ³¨æ„åŠ›å±‚æ•°å’Œæ›´å¤§çš„è¯å‘é‡ç»´åº¦ï¼Œä½ å°±èƒ½å¾—åˆ°ä¸€ä¸ªéå¸¸ä¸é”™çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ¯”å¦‚å¦‚æœå–œæ¬¢è¯—è¯çš„è¯ï¼Œå¯ä»¥ç”¨è¯—è¯è®­ç»ƒè¯—è¯ç”Ÿæˆæ¨¡å‹ï¼Œå–œæ¬¢çº¢æ¥¼æ¢¦ï¼Œå¯ä»¥ç”¨çº¢æ¥¼æ¢¦è®­ç»ƒçº¢æ¥¼æ¢¦é£æ ¼æ–‡å­—ç”Ÿæˆæ¨¡å‹ã€‚

### æ€»ç»“

å†æ€»ç»“ä¸€ä¸‹ç”Ÿæˆæ¨¡å‹å’Œç¿»è¯‘æ¨¡å‹ï¼ˆå½“ç„¶ä¸åªæ˜¯ç¿»è¯‘ï¼Œè€Œæ˜¯ä»»æ„çš„`seq2seqä»»åŠ¡`ï¼‰ï¼š

- ç”Ÿæˆæ¨¡å‹

```
Input
  â”‚
  â–¼
[Masked Self-Attention]  â†â”€ ä½¿ç”¨ tgt_mask
  â”‚
  â–¼
[FFN]
  â”‚
  â–¼
Output


```

- ç¿»è¯‘æ¨¡å‹

```
Input
  â”‚
  â–¼
[Masked Self-Attention]  â†â”€ ä½¿ç”¨ tgt_mask
  â”‚
  â–¼
[Cross-Attention]        â†â”€ ä½¿ç”¨ encoder è¾“å‡ºä½œä¸º K, V
  â”‚
  â–¼
[FFN]
  â”‚
  â–¼
Output
```
è¿˜æœ‰ï¼Œè™½ç„¶è¯´å•`decoder`æ¨¡å‹é€‚åˆç”Ÿæˆä»»åŠ¡ï¼Œä½†æ˜¯ä¸ä»£è¡¨ä¸èƒ½åšåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ç¿»è¯‘ï¼Œä¹Ÿæ˜¯å¯ä»¥åšçš„ã€‚åªæ˜¯æ¢ä¸ªæ€è·¯ï¼Œå°†åŸå§‹è¯­æ–™å½“æˆå¼è¾“å…¥çš„æç¤ºè¯ã€‚æ¯”å¦‚â€œæˆ‘çˆ±ä½ â€ç¿»è¯‘æˆâ€œI love youâ€ï¼ŒæŠŠæˆ‘çˆ±ä½ ä½œä¸ºæç¤ºè¯ï¼Œè¯å‘é‡åŒ–+ä½ç½®ç¼–ç åè¾“å…¥maskå¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œæ›´é«˜æ•ˆçš„æ–¹å¼æ˜¯å†åŠ ä¸€ä¸ªæ ‡å¿—ä½ï¼Œæ¯”å¦‚<ç¿»è¯‘>,æˆ–è€…æ›´å…·ä½“çš„<ä¸­è‹±ç¿»è¯‘>,è¿™æ ·æ¨¡å‹å¯ä»¥æ›´é«˜æ•ˆçš„è®­ç»ƒï¼Œç„¶åä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒå’Œâ€œIâ€è¿™ä¸ªlabelåšäº¤å‰ç†µæŸå¤±ï¼Œç„¶åæ›´æ–°å‚æ•°ï¼Œæ¥ç€è¾“å…¥maskåç§»å˜æˆâ€œæˆ‘çˆ±ä½  Iâ€ï¼Œç»§ç»­è®­ç»ƒï¼Œä¸‹ä¸ªè¯æ¦‚ç‡åˆ†å¸ƒå†å’Œâ€œloveâ€åšäº¤å‰ç†µæŸå¤±ï¼Œä¸€ç›´é‡å¤ï¼Œå°±å¯ä»¥è¿›è¡Œè®­ç»ƒã€‚æ¨ç†åŒç†ï¼Œåªéœ€è¦æŠŠåŸå§‹è¯­æ–™+<ç¿»è¯‘>æ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼Œå¾—åˆ°ä¸‹ä¸€è¯åˆ†å¸ƒåé‡‡æ ·ï¼Œç»“æœåŠ å…¥è¾“å…¥åºåˆ—ï¼Œé‡å¤ï¼Œç›´åˆ°åœæ­¢ä½å³å¯ã€‚










