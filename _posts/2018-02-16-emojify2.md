---
layout: post
title: emojify系列（二）
description: 利用pytorch实现emojify
---

# 模型构建

## 预训练词向量导入

上一篇博文里，我们已经完成了数据集的制作，接下来的一个问题就是，模型的构建，第一个问题就是词向量的问题，
我们已经有了一个训练好的词向量，那么在keras中很好处理，可以看我的[gitghub keras 实现](https://github.com/cryer/Emojify),
那么pytorch是怎么实现与训练向量的导入的呢，如果看官方文档，查看完整参数列表，发现似乎没有参数可以实现
预训练向量的导入，只有定义好就是随机的初始化向量。查看了源码，发现了存储embeding权重的变量就是weights，然后我
尝试直接修改weights的值，踩了几个坑，最后采用如下实现：
```
        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)
        weight = np.array(weight)
        self.word_embeds.weight.data.copy_(torch.from_numpy(weight))
```

ok,测试一下，怎么测试呢，随便找个词输入可以查看50维度的词向量，输入到上面的embeding层中，再输入到一个
没有赋值的embeding层中，通过查看词向量是否一致来看是不行的，因为embeding层每次都会随机选取初始化向量的，
不一致才是正常的，因此我们可以通过执行两次上面赋值后embeding，看看是否一样，一样说明赋值成功了，否则的话
随机变化肯定是不会一样的。测试代码如下：
```
vocab_len = len(word_to_index) + 1  #word index begin with 1,plus 1 for padding 0
emb_dim = word_to_vec_map["cucumber"].shape[0]
emb_matrix = np.zeros((vocab_len, emb_dim))
for word, index in word_to_index.items():
    emb_matrix[index, :] = word_to_vec_map[word]

word_embeds = nn.Embedding(vocab_len, 50)
pretrained_weight = np.array(emb_matrix)
word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))

input = Variable(torch.LongTensor([word_to_index["cucumber"]]))

print("111：",word_embeds(input))
print("222：",word_embeds(input))
print("333：",word_embeds(input))
```
为了保险，我输入了三次，结果都是一样的，这就说明赋值是成功的。

## 固定词向量

我们知道，训练时，我们是不希望词向量的值改变的，因为这个向量已经在很大的语料库上训练好了，不需要改变，
因此就涉及到固定子层参数不更新的技巧。这个方法有很多，常用的是requires_grad和volatile，requires_grad = False等同于
volatile = True。我就只说说前者了，每个Variable都有这个参数，自然就包括model的parameter，因此我们可以设置
embeding层的参数的parameter的requires_grad为False，这样就不会计算他们的梯度了，这样还会提升模型训练的速度，
因为预训练的词向量矩阵还是维度很大的，如果每次反向传播都计算它的梯度会极大地影响训练的速度。当然这样还是不够的，
设置优化器的时候，第一个参数就不可以是model。parameter()，因为embeding层的梯度是没有的，也就优化不了，因此要将它排除。
由于这里的层很少，因此我采用如下方法：
```
optimizer1 = torch.optim.Adam(model.rnn.parameters(),lr=0.001)
optimizer2 = torch.optim.Adam(model.linear.parameters(),lr=0.001)
```
就是用两个优化器，分别优化剩下的两个层，讲道理这种写法我都没在其他地方看过，应该是我看的代码比较少，可能有吧。
这个方法也是我自己想的，不知道有没有什么bug。那么自然，后面更新的step，也要两个优化器一起step。
完整代码我会上传到我的github的。

## 模型搭建

剩下的就蛮简单的了，注意好好看官方文档对应的API，每个参数都了解，能节省代码就节省代码，比如2层LSTM中间接dropout，
keras是依次实现的，而pytorch可以直接一个函数实现。具体结构如下：

![](https://github.com/cryer/Emojify/raw/master/image/emojifier-v2.png)

## 其他

* 记得还有一个问题是，读取自定义数据集时，会把数据变成DoubleTensor，target变成了intTensor,转化成longTensor就是
直接在后面加上.long()，就行了，其他的Tensor之间的转化也如此，.int(),.float(),等等。
* 完整代码地址 [pyTorch emojify](https://github.com/cryer/emojify-pyTorch)
