---
layout: post
title: å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„stable diffusion
description: å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„stable diffusion

---
<script>
  MathJax = {
    tex: {
      inlineMath: [['\$', '\$']],
      displayMath: [['\$', '\$']]
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

### å¯¼å…¥

ç›¸ä¿¡å¤§å®¶æœ€è¿‘éƒ½è¢«`stable diffusion`åˆ·å±äº†ï¼Œæ— è®ºæ˜¯ä¸æ˜¯ç ”ç©¶AIçš„ï¼Œä¸å¦‚è¯´æ›´å¤§éƒ¨åˆ†éƒ½æ˜¯AIé¢†åŸŸä¹‹å¤–çš„äººï¼Œéƒ½å¤šå°‘ä½“éªŒè¿‡äº†ï¼Œç›¸ä¿¡ä¹Ÿä¸ºè¿™ä¸ªå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹çš„æ•ˆæœæ‰€éœ‡æƒŠã€‚ä»åå­—ä¹Ÿå¯ä»¥çœ‹å‡º`stable diffusion`è‡ªç„¶æ˜¯åŸºäº`diffusion`æ‰©æ•£æŠ€æœ¯çš„ï¼Œå…³äºæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡æ–‡ç« ä»å¤´å¼€å§‹å®ç°æ‰©æ•£æ¨¡å‹ï¼Œä¹Ÿåœ¨æ–‡ç« ä¸­ç®€å•è¯´æ˜äº†æ‰©æ•£æ¨¡å‹çš„ä¸€äº›æ•°å­¦åŸç†å’Œå…¬å¼æƒ…å†µï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥å…ˆå»çœ‹çœ‹ã€‚å…¶å®åœ¨`stable diffusion`å­˜åœ¨å¦ä¸€ä¸ªå’Œæ‰©æ•£æ¨¡å‹åŒæ ·é‡è¦çš„æ¨¡å‹ï¼Œåªä¸è¿‡è¿æ°”ä¸å¥½ï¼Œæ²¡å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œé‚£å°±æ˜¯`CLIP`ï¼Œä¹Ÿå°±æ˜¯`Contrastive Language-Image Pretraining`,æ˜¯ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ï¼Œä½¿ç”¨2ä¸ªç¼–ç å™¨ï¼Œåˆ†åˆ«æ˜¯å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œè®©å›¾åƒå’Œæ–‡æœ¬ç¼–ç åçš„è¯­ä¹‰ç©ºé—´å°½å¯èƒ½æ¥è¿‘ï¼Œè¿™ä¹Ÿæ˜¯`stable diffusion`çš„æ ¸å¿ƒæŠ€æœ¯ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰`VAE`å˜åˆ†è‡ªç¼–ç å™¨çš„ä½¿ç”¨ï¼Œ`VAE`æ˜¯å¤šå¹´å‰çš„æ¨¡å‹äº†ï¼Œæˆ‘çš„`github`ä¸­ä¹Ÿæœ‰ä»å¤´å®ç°çš„`VAE`ä»£ç ï¼Œæœ¬èº«å¹¶ä¸å¤æ‚ï¼Œ`VAE`è¿™é‡Œä¸»è¦è´Ÿè´£æŠŠå›¾åƒåœ¨éšç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´æ¥å›è½¬æ¢ï¼Œå› ä¸ºè™½ç„¶åŸæ¥çš„æ‰©æ•£æ¨¡å‹æ˜¯åœ¨å›¾åƒæ§ä»¶ä¸‹è¿›è¡ŒåŠ å™ªæ‰©æ•£çš„ï¼Œä½†æ˜¯`stable diffusion`æ˜¯åœ¨éšç©ºé—´ä¹‹ä¸­å»åŠ å™ªæ‰©æ•£çš„ï¼Œè¿™æ ·ä¸ä»…åŠ é€Ÿäº†è®­ç»ƒï¼ŒèŠ‚çœäº†å‡ åå€çš„æ˜¾å­˜éœ€æ±‚ï¼ŒåŒæ—¶éšç©ºé—´å…¶å®æ›´å¤šçš„æ˜¯"è¯­ä¹‰â€œç©ºé—´ï¼Œæ›´åŠ å¹³æ»‘ï¼Œä¹Ÿæ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤å¯ä»¥å­¦ä¹ åˆ°â€è¯­ä¹‰çº§åˆ«â€œçš„å™ªå£°ã€‚å¦å¤–ï¼Œé™¤äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸­çš„**æ—¶é—´æ­¥åµŒå…¥**ä¹‹å¤–ï¼ŒSDä¸­`U-Net`è¿˜éœ€è¦åŠ å…¥æ–‡æœ¬åµŒå…¥ï¼Œä¹Ÿå°±æ˜¯`CLIP`ç¼–ç åçš„åµŒå…¥ï¼Œä½¿ç”¨çš„æ˜¯`cross-attention`åµŒå…¥æ–¹æ³•ï¼Œå›¾åƒéšç©ºé—´ä¿¡æ¯ä½œä¸º`Q`ï¼Œæ–‡æœ¬åµŒå…¥ä¿¡æ¯ä½œä¸º`KV`ã€‚æ›´å¤šçš„ç»†èŠ‚ï¼Œåœ¨åé¢ç»™å‡ºçš„ä»£ç ä¸­éƒ½å¯ä»¥çœ‹å¾—å¾ˆæ¸…æ¥šï¼Œæˆ‘ä¸å†è¿‡å¤šè¯´æ˜ï¼Œåªæ˜¯ä¸‹é¢ç®€å•è¯´æ˜ä¸‹SDçš„æ•´ä½“æ¶æ„å’Œè®­ç»ƒæ¨ç†æµç¨‹ã€‚

**æ•´ä½“æ¶æ„**

```
æ–‡æœ¬æç¤º (Text Prompt)
        â†“
[CLIP Text Encoder] â†’ æ–‡æœ¬åµŒå…¥ (Text Embedding)
        â†“
[U-Net Denoising Model] â† å™ªå£°æ½œåœ¨è¡¨ç¤º (Noisy Latent)
        â†“
[VAE Decoder] â†’ ç”Ÿæˆå›¾åƒ (Generated Image)
```

**è®­ç»ƒè¿‡ç¨‹**ï¼š

**Step 1: VAE ç¼–ç **

- å°†çœŸå®å›¾åƒ xâ€‹ é€šè¿‡ VAE ç¼–ç å™¨å¾—åˆ°æ½œåœ¨è¡¨ç¤º x0â€‹

**Step 2: å™ªå£°æ·»åŠ **

- éšæœºé€‰æ‹©æ—¶é—´æ­¥ tâˆ¼Uniform(0,T)
- é‡‡æ ·å™ªå£° Ïµâˆ¼N(0,I)
- è®¡ç®—å¸¦å™ªæ½œåœ¨è¡¨ç¤ºï¼š\$X_t = \sqrt{a_t } X_{t-1}+ \sqrt{1 - a_t} \epsilon\$ 

**Step 3: æ–‡æœ¬ç¼–ç **

- å°†æ–‡æœ¬æç¤ºé€šè¿‡ CLIP æ–‡æœ¬ç¼–ç å™¨å¾—åˆ°åµŒå…¥ c

**Step 4: å™ªå£°é¢„æµ‹**

- è¾“å…¥ (xtâ€‹,t,c) åˆ° U-Net
- è¾“å‡ºå™ªå£°é¢„æµ‹ ÏµÎ¸â€‹(xtâ€‹,t,c)

**Step 5: æŸå¤±è®¡ç®—å’Œä¼˜åŒ–**

- è®¡ç®— MSE æŸå¤±ï¼šL=âˆ¥Ïµâˆ’ÏµÎ¸â€‹âˆ¥2
- ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨æ›´æ–° U-Net å‚æ•°
- VAE å’Œ CLIP å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å›ºå®š

**æ¨ç†è¿‡ç¨‹**ï¼š

**Step 1: æ–‡æœ¬ç¼–ç **

- è¾“å…¥æ–‡æœ¬æç¤º â†’ CLIP æ–‡æœ¬ç¼–ç å™¨ â†’ æ–‡æœ¬åµŒå…¥ c

**Step 2: åˆå§‹åŒ–å™ªå£°**

- ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·ï¼šxTâ€‹âˆ¼N(0,I)
- å°ºå¯¸ï¼šæ¯”å¦‚ 64Ã—64Ã—4ï¼ˆå¯¹åº” 512Ã—512 å›¾åƒï¼‰

**Step 3: è¿­ä»£å»å™ª**

- å¯¹äº t=T,Tâˆ’1,...,1 ï¼š
  1. è¾“å…¥ (xtâ€‹,t,c) åˆ° U-Net
  2. è·å¾—å™ªå£°é¢„æµ‹ ÏµÎ¸â€‹
  3. ä½¿ç”¨é‡‡æ ·å™¨è®¡ç®— xtâˆ’1â€‹

**Step 4: å›¾åƒé‡å»º**

- å°†æœ€ç»ˆæ½œåœ¨è¡¨ç¤º x0â€‹ è¾“å…¥ VAE è§£ç å™¨
- è¾“å‡º æ¯”å¦‚ 512Ã—512Ã—3 çš„ç”Ÿæˆå›¾åƒ

å…³äºé‡‡æ ·å™¨ï¼Œå†å¤šè¯´ä¸€äº›ï¼Œå¦‚æœä½ çœ‹äº†æˆ‘ä¹‹å‰æ‰©æ•£æ¨¡å‹çš„æ–‡ç« ï¼Œæˆ–è€…äº†è§£æ‰©æ•£æ¨¡å‹ï¼Œåº”è¯¥çŸ¥é“ä¹‹å‰çš„`DDPM`é‡‡æ ·ï¼Œæ˜¯é‡‡æ ·1000æ­¥ï¼Œæ¯æ­¥æ¨¡å‹è¾“å‡ºçš„å™ªéŸ³ï¼Œæ¥æ ¹æ®å…¬å¼è®¡ç®—å‡å€¼ï¼Œç„¶åå†ä½¿ç”¨`Î²`ä½œä¸ºæ–¹å·®ï¼Œå¼•å…¥éšæœºå‡å€¼0æ–¹å·®1çš„å™ªå£°ï¼Œåˆ©ç”¨é‡å‚æ•°åŒ–æ„å»ºå›¾åƒæ•°æ®ï¼Œç¬¬1000æ­¥ä¸ç”¨æ·»åŠ å™ªå£°ï¼Œç›´æ¥è·å–é‡‡æ ·å®Œæˆçš„å›¾åƒã€‚è¿™ä¸ªå±äºæ˜¯åŸå§‹çš„ç†è®ºå…¬å¼ï¼Œè¿™é‡ŒSDä¸­å½“ç„¶æ˜¯å¯ä»¥ä½¿ç”¨çš„ï¼Œä½†æ˜¯ç¼ºç‚¹å¾ˆæ˜æ˜¾ï¼š

- é€Ÿåº¦ææ…¢ï¼ˆåˆ†é’Ÿçº§ï¼‰

- æ¯æ­¥åŠ å™ªï¼Œéšæœºæ€§è¿‡å¤§ï¼Œä¸å¥½æ§åˆ¶

å•æ˜¯é€Ÿåº¦å¤ªæ…¢ï¼Œåœ¨å·¥ç¨‹ä¸Šå°±å·²ç»ä¸æ€ä¹ˆè€ƒè™‘äº†ï¼Œè€Œæ›´ç°ä»£çš„æ–¹æ³•ï¼Œæ¯”å¦‚`DDIM`ï¼Œ`DPM`å¯ä»¥ç”¨å°å‡ åæ­¥å°±èƒ½è·å–å‡ ä¹å’Œ`DDPM`é‡‡æ ·1000æ­¥å·®ä¸å¤šè´¨é‡çš„å›¾åƒã€‚ç°ä»£çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ•°å€¼é€¼è¿‘çš„æ–¹æ³•ï¼Œé‡‡ç”¨ä¸€é˜¶ï¼ŒäºŒé˜¶æˆ–è€…æ›´é«˜é˜¶çš„æ–¹æ³•ï¼Œæœ‰çš„æ˜¯ç¡®å®šæ€§é‡‡æ ·ï¼Œæœ‰çš„ä¹Ÿéœ€è¦åŠ å™ªï¼Œå¯ä»¥æŒ‰éœ€é€‰æ‹©ã€‚è€Œæ•°å€¼é€¼è¿‘æ•ˆæœå¥½çš„åŸå› ï¼Œä¹Ÿè·ŸSDæ˜¯åœ¨éšç©ºé—´æ‰©æ•£æœ‰å…³ï¼Œéšç©ºé—´è¿™ç§æ›´åŠ å¹³æ»‘çš„â€è¯­ä¹‰çº§åˆ«â€œçš„åŠ å™ªï¼Œä½¿å¾—è·³è·ƒæ­¥æ•°çš„é‡‡æ ·é€¼è¿‘æ•ˆæœä¹Ÿå¾ˆä¸é”™ã€‚åè¿‡æ¥ï¼Œä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥é‡‡ç”¨ç°ä»£é‡‡æ ·å™¨ï¼Œå³ä½¿æ˜¯åœ¨å›¾åƒç©ºé—´ï¼Œæ•ˆæœä¹Ÿä¸ç®—å·®ï¼Œé€Ÿåº¦å´æ˜¯è´¨çš„é£è·ƒã€‚2021å¹´æå‡º`DDIM`çš„è®ºæ–‡`Denoising Diffusion Implicit Models`ä¸­å°±å±•ç¤ºäº†åœ¨` CIFAR10ã€CelebAã€LSUN `ä¸Šï¼ŒDDIM ç”¨ 100 æ­¥å°±èƒ½åŒ¹é… DDPM 1000 æ­¥è´¨é‡ã€‚

### å®Œæ•´ä»£ç 

#### é¡¹ç›®ç»“æ„

```
stable_diffusion_toy/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ images/          # å­˜æ”¾æ•°æ®é›†å›¾ç‰‡
â”‚   â””â”€â”€ captions.txt     # å¯¹åº”æ–‡æœ¬æè¿°
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ unet.py          # è‡ªå®šä¹‰U-Net
â”‚   â””â”€â”€ diffusion.py      # æ‰©æ•£æ¨¡å‹ä¸»ç±»
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py       # æ•°æ®é›†åŠ è½½
â”‚   â””â”€â”€ ddim.py          # DDIMé‡‡æ ·å™¨
â”œâ”€â”€ train.py             # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py         # æ¨ç†è„šæœ¬
â””â”€â”€ config.py            # é…ç½®æ–‡ä»¶
```

é¦–å…ˆè¦è¯´æ˜çš„æ˜¯ï¼Œå› ä¸ºæ˜¯ç©å…·é¡¹ç›®ï¼Œä¸ä¼šä½¿ç”¨å¾ˆå¤§çš„æ¨¡å‹ï¼Œæ›´å¤šçš„æ•°æ®ï¼Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ‰€ä»¥æ•ˆæœä¸ä¼šå¾ˆå¥½ï¼Œé‡ç‚¹å…³æ³¨åº•å±‚åŸç†ï¼Œä½ è¦çŸ¥é“å®˜æ–¹çš„SDä½¿ç”¨`LAION-5B`æ•°æ®é›†ï¼ŒåŒ…å«50äº¿å¯¹å›¾åƒæ–‡æœ¬æ ·æœ¬ï¼Œç„¶ååœ¨æ•°ç™¾A100 GPUä¸Šè®­ç»ƒçš„ï¼Œè€Œæœ¬é¡¹ç›®å°†ä¼šä½¿ç”¨è‡ªå·±ç½‘ä¸Šçˆ¬å–çš„1000ä¸ªå›¾ç‰‡æ ·æœ¬ï¼Œå›¾ç‰‡å‘½å1.jpg~1000.jpgï¼Œä¸»è¦æ˜¯äººç‰©å’Œäº¤é€šå·¥å…·ï¼Œç„¶åæ–‡æœ¬æè¿°åŒæ ·æ˜¯1000è¡Œï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªå›¾ç‰‡ï¼Œè®­ç»ƒä¸ª50è½®ï¼Œä¸”æ¨¡å‹å°ï¼Œæ•ˆæœè‡ªç„¶ä¸ç”¨å¥¢æ±‚å¤ªå¤šï¼Œè¿™ç§é¡¹ç›®ä¹Ÿä¸æ˜¯æ™®é€šäººèƒ½ä»å¤´è®­ç»ƒçš„èµ·çš„ã€‚`VAE`å’Œ`CLIP`åˆ™ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ•´ä¸ªSDè®­ç»ƒè¿‡ç¨‹ï¼Œè¿™ä¸¤ä¸ªçš„å‚æ•°éƒ½æ˜¯å›ºå®šçš„ã€‚

> æ•°æ®é›†æˆ‘å°±ä¸ç»™å‡ºäº†ï¼Œæˆ‘çˆ¬å–çš„æ•°æ®é›†è´¨é‡å¾ˆå·®ï¼Œä¸”æ–‡æœ¬æè¿°ä¹Ÿæ˜¯æ‰¹é‡çš„é‚£ç§ï¼Œå¦‚æœçœŸçš„æƒ³è‡ªå·±è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨LAION-5Bå…¬å¼€æ•°æ®é›†ï¼Œä»ä¸­é€‰1000å¼ ï¼Œæˆ–è€…æ›´å¤šå³å¯ï¼ŒæŒ‰ç…§æˆ‘è¯´çš„æ ¼å¼å³å¯ï¼Œä¹Ÿå°±æ˜¯å›¾ç‰‡å‘½å1.jpg~1000.jpgï¼Œæ–‡æœ¬æè¿°1000è¡Œï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªå›¾ç‰‡

**unet.py**

ç®€åŒ–ç‰ˆu-netï¼Œæ¨¡å‹å±‚æ•°ä¸é«˜ï¼Œå’Œä¼ ç»Ÿ`diffusion`æ‰©æ•£æ¨¡å‹ä¸­çš„u-netçš„ä¸€ä¸ªåŒºåˆ«æ˜¯ç“¶é¢ˆå±‚é™¤äº†ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å¯¹æ–‡æœ¬ç¼–ç åµŒå…¥å¤„ç†ä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨äº†ç©ºé—´è‡ªæ³¨æ„åŠ›å¯¹è‡ªèº«å›¾åƒç©ºé—´ï¼ˆéšç©ºé—´ï¼‰çš„ç©ºé—´å…³ç³»è¿›è¡Œå¤„ç†ï¼Œä½†æ˜¯è¿™å…¶å®ä¹Ÿåªæ˜¯é’ˆå¯¹æœ€å¼€å§‹çš„æ‰©æ•£æ¨¡å‹ï¼Œå®é™…ä¸Šåæ¥çš„æ‰©æ•£æ¨¡å‹ä¸ºäº†è¿›ä¸€æ­¥æé«˜å›¾åƒåˆ†è¾¨ç‡ç”Ÿæˆè´¨é‡ï¼Œæ˜¯æ—©å°±æœ‰åŠ å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ï¼Œå°¤å…¶æ˜¯åœ¨ç©ºé—´åˆ†è¾¨ç‡è¾ƒä½çš„å±‚åŠ å…¥è‡ªæ³¨æ„åŠ›å±‚ï¼Œå¯ä»¥éå¸¸æœ‰æ•ˆæ•è·å›¾åƒçš„ç©ºé—´ç‰¹å¾ã€‚ç©ºé—´è‡ªæ³¨æ„åŠ›è¿˜æ˜¯å±äºæ— æ¡ä»¶ç”Ÿæˆï¼Œè€Œäº¤å‰æ³¨æ„åŠ›å±äºæœ‰æ¡ä»¶ç”Ÿæˆï¼Œæ‰€ä»¥ä¾èµ–æ–‡æœ¬ç¼–ç æ¡ä»¶ï¼Œè€Œæ–‡æœ¬ç¼–ç å¦‚æœç®€å•çš„é€šè¿‡æ‹¼æ¥æˆ–è€…ç›¸åŠ å°±ä¼šç ´åå›¾åƒç©ºé—´çš„ç©ºé—´ç»“æ„ï¼ŒCross-Attention åœ¨ä¿æŒå›¾åƒç©ºé—´ç»´åº¦çš„åŒæ—¶å¼•å…¥è¯­ä¹‰ï¼Œå¯ä»¥åšåˆ°å›¾åƒä¸åŒç©ºé—´åŒºåŸŸå…³æ³¨ä¸åŒçš„æç¤ºè¯ï¼Œè€Œç›¸åŠ æ˜¾ç„¶å°±æ˜¯å…¨å±€çš„ï¼Œæ— æ³•åŒºåˆ†ä¸åŒåŒºåŸŸåº”å…³æ³¨ä¸åŒè¯ã€‚å½“ç„¶äº¤å‰æ³¨æ„åŠ›å±‚ä¹Ÿæ˜¯ç®€åŒ–ç‰ˆï¼Œæˆ‘ä¹‹å‰å†™è¿‡ä»å¤´å®ç°`transformers`çš„æ–‡ç« ï¼Œé‡Œé¢å¯¹transformersæ¶æ„çš„å®ç°æ›´è¯¦ç»†ä¸€äº›ï¼Œæƒ³è¦æ›´äº†è§£çš„å¯ä»¥å»çœ‹ä¸€ä¸‹ã€‚

ä¸ºäº†è®©è¯»è€…æ›´æ¸…æ¥šäº¤å‰æ³¨æ„åŠ›å±‚çš„åµŒå…¥ï¼Œæˆ‘å†è¯¦ç»†è¯´æ˜ä¸‹ï¼Œå‡è®¾å›¾åƒç‰¹å¾ï¼ˆQueryï¼‰ï¼šx ç»´åº¦`[B, L, C]`ï¼Œå…¶ä¸­ L = HÃ—Wï¼ŒC = 256,æ–‡æœ¬ç¼–ç ï¼ˆKey/Valueï¼‰ï¼šcontext ç»´åº¦`[B, N, D]`ï¼Œå…¶ä¸­ N = 77ï¼ˆCLIP token æ•°ï¼‰ï¼ŒD = 768ã€‚é¦–å…ˆé€šè¿‡æ˜ å°„æƒé‡çŸ©é˜µ`to_q,to_k,to_v`æŠ•å½±åˆ°ç›¸åŒçš„æ³¨æ„åŠ›å¤´ç»´åº¦ç©ºé—´ï¼Œæ¯”å¦‚8å¤´æ³¨æ„åŠ›ï¼Œæ¯å¤´ç»´åº¦32ï¼Œé‚£ä¹ˆæŠ•å½±ç»´åº¦å°±æ˜¯32x8=256ï¼Œæ­¤æ—¶ï¼što_q: `[B, L, 256] â†’ [B, L, 256]` to_k: `[B, 77, 768] â†’ [B, 77, 256]` to_v: `[B, 77, 768] â†’ [B, 77, 256]`
ç„¶åå¤šå¤´æ‹†åˆ†åï¼Œç»´åº¦ä¾æ¬¡å˜æˆ`[B, 8, L, 32],[B, 8, 77, 32],[B, 8, 77, 32]`ï¼ˆäº¤æ¢äº†ä¸‹ç»´åº¦ï¼Œå°†å¤´ç»´åº¦å‰ç§»ï¼‰,ç„¶åqkå¼€å§‹è®¡ç®—ç›¸ä¼¼åº¦ï¼Œ`sim = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale`,æ­¤æ—¶ç»´åº¦å°±æ˜¯`q[B, 8, L, 32],k[B, 8, 77, 32] -> sim[B, 8, L, 77]`,**è¡¨ç¤ºå¯¹äº batch ä¸­æ¯ä¸ªæ ·æœ¬ã€æ¯ä¸ªæ³¨æ„åŠ›å¤´ã€å›¾åƒçš„æ¯ä¸ªä½ç½®ï¼ˆå…± L ä¸ªï¼‰ï¼Œè®¡ç®—å®ƒä¸ 77 ä¸ªæ–‡æœ¬ token çš„ç›¸ä¼¼åº¦**ã€‚ç„¶åsoftmaxå½’ä¸€åŒ–ä¹‹åå°±æ˜¯**å¾—åˆ°æ¯ä¸ªå›¾åƒä½ç½®å¯¹æ¯ä¸ªæ–‡æœ¬ token çš„æ³¨æ„åŠ›æƒé‡**ï¼Œç„¶ååŠ æƒèšåˆï¼ˆAttn Ã— Vï¼‰ä¹‹åï¼Œç»´åº¦å˜åŒ–`æƒé‡[B, 8, L, 77]ï¼Œv[B, 8, 77, 32]  -> [B, 8, L, 32]` ,**æ¯ä¸ªå›¾åƒä½ç½®å¾—åˆ°ä¸€ä¸ª èåˆäº†æ–‡æœ¬è¯­ä¹‰çš„ 32 ç»´å‘é‡ï¼ˆæ¯ä¸ªå¤´ï¼‰**ï¼Œè¿™ä¹Ÿæ­£æ˜¯ä¸Šé¢è¯´çš„å›¾åƒä¸åŒç©ºé—´åŒºåŸŸå…³æ³¨ä¸åŒçš„æç¤ºè¯çš„æ¥æºï¼Œç„¶åå¤´åˆå¹¶å°±æ²¡ä»€ä¹ˆå¥½è¯´çš„äº†ã€‚

> å®é™…çš„SDä¸­çš„u-netçš„ç¼–ç è§£ç æ¨¡å—åˆ†ä¸ºå¤šä¸ªå±‚çº§ï¼Œè¡¨ç¤ºä¸åŒçš„åˆ†è¾¨ç‡ï¼ˆå¦‚ 64Ã—64, 32Ã—32, 16Ã—16, 8Ã—8ï¼‰ ï¼Œæ¯ä¸ªå±‚çº§çš„resnetå—ä¹‹åéƒ½ä¼šåŠ å…¥ä¸€ä¸ªç©ºé—´è‡ªæ³¨æ„åŠ›å±‚å’Œäº¤å‰æ³¨æ„åŠ›æ–‡æœ¬åµŒå…¥å±‚ï¼Œæœ¬æ–‡ç®€åŒ–ç‰ˆåªåŠ åœ¨ç“¶é¢ˆå±‚ã€‚

```python
# unet.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint

class TimestepEmbedding(nn.Module):
    """æ—¶é—´æ­¥åµŒå…¥å±‚"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.half_dim = dim // 2
        # ç¬¬ä¸€ä¸ª Linear è¾“å…¥åº”ä¸º half_dimï¼Œè¾“å‡º dim
        self.emb = nn.Sequential(
            nn.Linear(self.half_dim, dim),  # è¾“å…¥æ˜¯ half_dim
            nn.SiLU(),
            nn.Linear(dim, dim)
        )

    def forward(self, t):
        # t: [B]
        t = t.float()
        half_dim = self.half_dim
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)  # [half_dim]
        emb = t[:, None] * emb[None, :]  # [B, half_dim]
        # è½¬æ¢ emb çš„ dtype ä»¥åŒ¹é… Linear å±‚
        emb = emb.to(self.emb[0].weight.dtype)  # è‡ªåŠ¨åŒ¹é… Linear çš„ dtype
        # ä¿ç•™ [B, half_dim]ï¼Œè®© Linear å±‚æ˜ å°„åˆ° dim
        return self.emb(emb)  # [B, half_dim] â†’ Linear â†’ [B, dim]

class CrossAttention(nn.Module):
    """ç®€åŒ–äº¤å‰æ³¨æ„åŠ›å±‚"""
    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):
        super().__init__()
        self.dim_head = dim_head  # ä¿å­˜ dim_head
        self.inner_dim = dim_head * heads
        context_dim = context_dim or query_dim

        self.scale = dim_head ** -0.5
        self.heads = heads
        self.to_q = nn.Linear(query_dim, self.inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, self.inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, self.inner_dim, bias=False)
        self.to_out = nn.Linear(self.inner_dim, query_dim)

    def forward(self, x, context=None):
        B, L, _ = x.shape  # x: [B, L, C]
        h = self.heads
        dim_head = self.dim_head  # æ˜¾å¼ä½¿ç”¨ä¿å­˜çš„ dim_head

        q = self.to_q(x)  # [B, L, 512]
        context = context if context is not None else x
        k = self.to_k(context)  # [B, 77, 512]
        v = self.to_v(context)  # [B, 77, 512]

        # åˆ†å¤´: [B, L, 512] -> [B, L, h, dim_head] -> [B, h, L, dim_head]
        q = q.view(B, L, h, dim_head).transpose(1, 2)
        k = k.view(B, context.shape[1], h, dim_head).transpose(1, 2) 
        v = v.view(B, context.shape[1], h, dim_head).transpose(1, 2)  

        # è®¡ç®—æ³¨æ„åŠ›
        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale  # [B, h, L, 77]
        attn = sim.softmax(dim=-1)  # [B, h, L, 77]

        # åŠ æƒæ±‚å’Œ
        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)  # [B, h, L, dim_head]

        # åˆå¹¶å¤´
        out = out.transpose(1, 2).contiguous().view(B, L, self.inner_dim)  # [B, L, 512]
        return self.to_out(out)

class ResnetBlock(nn.Module):
    """å¸¦æ—¶é—´æ¡ä»¶çš„æ®‹å·®å—"""
    def __init__(self, in_c, out_c, time_emb_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_emb_dim, out_c)
        )
        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)
        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)
        self.norm1 = nn.GroupNorm(min(8, in_c), in_c)  # æœ€å¤š8ç»„ï¼Œä½†ä¸è¶…è¿‡é€šé“æ•°
        self.norm2 = nn.GroupNorm(min(8, out_c), out_c)
        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()

    def forward(self, x, t):
        if x.shape[1] != self.norm1.num_channels:
            raise ValueError(f"é€šé“æ•°ä¸åŒ¹é…ï¼è¾“å…¥: {x.shape[1]}, æœŸæœ›: {self.norm1.num_channels}")

        def block1(x_in):
            return self.conv1(F.silu(self.norm1(x_in)))

        def block2(x_in):
            return self.conv2(F.silu(self.norm2(x_in)))

        h = checkpoint(block1, x)
        h += self.mlp(t)[:, :, None, None]
        h = checkpoint(block2, h)
        return h + self.shortcut(x)

class AttentionBlock(nn.Module):
    """ç©ºé—´è‡ªæ³¨æ„åŠ›å—"""
    def __init__(self, dim):
        super().__init__()
        self.norm = nn.GroupNorm(8, dim)
        self.attn = nn.MultiheadAttention(dim, 4, batch_first=True)

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.norm(x)
        h = h.view(B, C, -1).transpose(1, 2)  # [B, H*W, C]
        h, _ = self.attn(h, h, h)
        h = h.transpose(1, 2).view(B, C, H, W)
        return x + h

class UNet(nn.Module):
    """ç®€åŒ–ç‰ˆæ¡ä»¶U-Netï¼Œæ”¯æŒæ–‡æœ¬äº¤å‰æ³¨æ„åŠ›"""
    def __init__(self, in_channels=4, out_channels=4, text_embed_dim=512):
        super().__init__()

        # æ—¶é—´æ­¥åµŒå…¥
        time_embed_dim = 256
        self.time_embed = TimestepEmbedding(time_embed_dim)
        self.time_mlp = nn.Sequential(
            nn.Linear(time_embed_dim, time_embed_dim),
            nn.SiLU(),
            nn.Linear(time_embed_dim, time_embed_dim)
        )

        # ç¼–ç å™¨å±‚
        self.enc1 = ResnetBlock(in_channels, 64, time_embed_dim)
        self.enc2 = ResnetBlock(64, 128, time_embed_dim)
        self.enc3 = ResnetBlock(128, 256, time_embed_dim)

        # ç“¶é¢ˆå±‚
        self.mid1 = ResnetBlock(256, 256, time_embed_dim)
        self.mid_attn = AttentionBlock(256)
        self.mid_cross_attn = CrossAttention(256, text_embed_dim)
        self.mid2 = ResnetBlock(256, 256, time_embed_dim)

        # è§£ç å™¨å±‚
        self.dec1 = ResnetBlock(256 + 128, 128, time_embed_dim)  
        self.dec2 = ResnetBlock(128 + 64, 64, time_embed_dim) 
        self.dec3 = ResnetBlock(64, 64, time_embed_dim) 

        # è¾“å‡ºå±‚
        self.out = nn.Sequential(
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            nn.Conv2d(64, out_channels, 3, padding=1)
        )

        # ä¸‹é‡‡æ · & ä¸Šé‡‡æ ·
        self.downsample = nn.MaxPool2d(2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

    def forward(self, x, t, context):
        # x: [B, C, H, W], t: [B], context: [B, L, 768]
        t_emb = self.time_mlp(self.time_embed(t))

        # ç¼–ç å™¨
        h1 = self.enc1(x, t_emb)  # [B, 64, H, W]
        h2 = self.enc2(self.downsample(h1), t_emb)  # [B, 128, H/2, W/2]
        h3 = self.enc3(self.downsample(h2), t_emb)  # [B, 256, H/4, W/4]

        # ç“¶é¢ˆ
        h = self.mid1(h3, t_emb)
        h = self.mid_attn(h)
        B, C, H, W = h.shape
        h_flat = h.view(B, C, -1).transpose(1, 2)  # [B, H*W, C]
        h_flat = self.mid_cross_attn(x=h_flat, context=context)
        h = h_flat.transpose(1, 2).view(B, C, H, W)
        h = self.mid2(h, t_emb)

        # è§£ç å™¨
        h = self.upsample(h)  
        h = torch.cat([h, h2], dim=1)               
        h = self.dec1(h, t_emb)                     

        h = self.upsample(h)                         
        h = torch.cat([h, h1], dim=1)                
        h = self.dec2(h, t_emb)                  

        h = self.dec3(h, t_emb) 
        out = self.out(h)        
        return out
```

**diffusion.py**

```python
# diffusion.py
import torch
import torch.nn.functional as F
from config import Config

class DiffusionModel:
    def __init__(self, unet, vae, text_encoder, config):
        self.unet = unet
        self.vae = vae
        self.text_encoder = text_encoder
        self.config = config
        self.device = config.device

        # é¢„è®¡ç®—beta schedule
        self.num_train_timesteps = config.num_train_timesteps
        self.betas = torch.linspace(0.0001, 0.02, self.num_train_timesteps)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)

        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.betas = self.betas.to(self.device)
        self.alphas_cumprod = self.alphas_cumprod.to(self.device)
        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(self.device)
        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(self.device)

    def add_noise(self, x_start, t):
        """å‰å‘åŠ å™ª"""
        noise = torch.randn_like(x_start)
        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)
        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)
        x_noisy = sqrt_alpha * x_start + sqrt_one_minus_alpha * noise
        return x_noisy, noise

    def get_loss(self, x_start, text_tokens, t):
        """è®¡ç®—æ‰©æ•£æŸå¤±"""
        # ç¼–ç æ–‡æœ¬
        with torch.no_grad():
            text_embeddings = self.text_encoder(text_tokens)[0]  # [B, 77, 768]


        # ç¼–ç å›¾åƒåˆ°latentç©ºé—´
        with torch.no_grad():
            latents = self.vae.encode(x_start).latent_dist.sample() * 0.18215  # VAEç¼©æ”¾å› å­


        # åŠ å™ª
        x_noisy, noise = self.add_noise(latents, t)


        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(x_noisy, t, text_embeddings)

        # MSEæŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        return loss
```

**dataset.py**

```python
# dataset.py
import os
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import CLIPTokenizer
from config import Config

class TextImageDataset(Dataset):
    def __init__(self, image_dir, caption_file, tokenizer, image_size=64):
        self.image_dir = image_dir
        self.captions = []
        self.image_names = []

        # è¯»å–captionæ–‡ä»¶
        with open(caption_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for i, line in enumerate(lines):
                caption = line.strip()
                image_name = f"{i+1}.jpg"  # å‡è®¾å›¾ç‰‡å‘½åä¸º 1.jpg, 2.jpg...
                image_path = os.path.join(image_dir, image_name)
                if os.path.exists(image_path):
                    self.captions.append(caption)
                    self.image_names.append(image_name)

        self.tokenizer = tokenizer
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1, 1]
        ])

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        img_path = os.path.join(self.image_dir, self.image_names[idx])
        image = Image.open(img_path).convert("RGB")
        image = self.transform(image)  # [C, H, W]

        # ç¼–ç æ–‡æœ¬
        text = self.captions[idx]
        tokens = self.tokenizer(
            text,
            padding="max_length",
            max_length=77,  # CLIPæœ€å¤§é•¿åº¦
            truncation=True,
            return_tensors="pt"
        )
        input_ids = tokens.input_ids.squeeze(0)  # [77]

        return image, input_ids

def get_dataloader(config):
    # åˆå§‹åŒ–CLIP tokenizer
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")  
    dataset = TextImageDataset(
        config.image_dir,
        config.caption_file,
        tokenizer,
        image_size=config.image_size  # åŸå§‹å›¾åƒå°ºå¯¸
    )

    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0
    )

    return dataloader, tokenizer
```

**ddim.py**

```python
# ddim.py
import torch

class DDIMSampler:
    def __init__(self, diffusion_model):
        self.diffusion = diffusion_model
        self.device = diffusion_model.device

    @torch.no_grad()
    def sample(self, text_tokens, batch_size=1, guidance_scale=7.5, num_steps=50):
        """DDIMé‡‡æ ·ç”Ÿæˆå›¾åƒ"""
        unet = self.diffusion.unet
        vae = self.diffusion.vae
        text_encoder = self.diffusion.text_encoder
        config = self.diffusion.config

        # ç¼–ç æ–‡æœ¬
        text_embeddings = text_encoder(text_tokens)[0]  # [B, 77, 768]
        uncond_tokens = torch.zeros_like(text_tokens)
        uncond_embeddings = text_encoder(uncond_tokens)[0]

        # åˆå¹¶æ¡ä»¶ä¸æ— æ¡ä»¶åµŒå…¥ï¼ˆCFGï¼‰
        context = torch.cat([uncond_embeddings, text_embeddings])

        # åˆå§‹åŒ–éšæœºå™ªå£°
        latents = torch.randn(
            (batch_size, config.latent_channels, config.latent_size, config.latent_size),  # ğŸ‘ˆ ä¿®å¤1ï¼šlatent_size
            dtype=torch.float32, 
            device=self.device
        )

        # DDIMæ—¶é—´æ­¥
        timesteps = torch.linspace(
            self.diffusion.num_train_timesteps - 1, 0, num_steps, dtype=torch.long
        ).to(self.device)

        for i, t in enumerate(timesteps):
            t_batch = t.repeat(batch_size * 2).to(latents.dtype) 

            # é¢„æµ‹å™ªå£°
            latent_model_input = torch.cat([latents] * 2)
            noise_pred = unet(latent_model_input, t_batch, context)

            # åˆ†ç¦»æ— æ¡ä»¶å’Œæ¡ä»¶é¢„æµ‹
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

            # DDIMæ›´æ–°å…¬å¼ 
            alpha_t = self.diffusion.alphas_cumprod[t].to(latents.dtype)
            alpha_t_prev = self.diffusion.alphas_cumprod[timesteps[i+1]].to(latents.dtype) if i < len(timesteps)-1 else torch.tensor(1.0, dtype=latents.dtype, device=self.device)

            # æ ‡å‡† DDIM å…¬å¼ï¼ˆsigma=0ï¼‰
            pred_x0 = (latents - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
            pred_x0 = torch.clamp(pred_x0, -1.0, 1.0)  

            # é‡æ„ latents
            latents = torch.sqrt(alpha_t_prev) * pred_x0 + torch.sqrt(1 - alpha_t_prev) * noise_pred

            # æ¸…ç†ç¼“å­˜
            latents = latents.detach()
            del noise_pred, noise_pred_uncond, noise_pred_text, pred_x0, dir_xt
            torch.cuda.empty_cache()

        # è§£ç latentåˆ°å›¾åƒ 
        latents = latents / 0.18215
        images = vae.decode(latents.float()).sample 
        images = (images / 2 + 0.5).clamp(0, 1)  # [-1,1] -> [0,1]

        return images
```

**config.py**

è®­ç»ƒå‚æ•°è®°å¾—æ ¹æ®ä½ å¾—å®é™…æ˜¾å­˜è¿›è¡Œè°ƒæ•´ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨colabè®­ç»ƒï¼Œcolabä¸Šä½¿ç”¨256çš„å›¾ç‰‡å°ºå¯¸å’Œ16çš„batchsizeåº”è¯¥é—®é¢˜ä¸å¤§ã€‚

```python
# config.py
import torch

class Config:
    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ•°æ®è·¯å¾„
    image_dir = "./dataset/images"
    caption_file = "./dataset/captions.txt"

    # æ¨¡å‹å‚æ•°
    latent_channels = 4          # VAEå‹ç¼©åé€šé“æ•°

    image_size = 256     # VAEå‹ç¼©åå›¾åƒå°ºå¯¸
    latent_size = image_size // 8 

    text_embed_dim = 512         # CLIPæ–‡æœ¬åµŒå…¥ç»´åº¦
    unet_in_channels = 4         # è¾“å…¥é€šé“ï¼ˆlatentï¼‰
    unet_out_channels = 4        # è¾“å‡ºé€šé“ï¼ˆé¢„æµ‹å™ªå£°ï¼‰

    # è®­ç»ƒå‚æ•°
    batch_size = 8
    epochs = 50
    learning_rate = 5e-5
    num_train_timesteps = 1000   # æ‰©æ•£æ­¥æ•°
    save_path = "./models/sd_toy_64.pth"

    # é‡‡æ ·å‚æ•°
    num_inference_steps = 50     # DDIMæ­¥æ•°
    guidance_scale = 7.5         # æ–‡æœ¬å¼•å¯¼å¼ºåº¦
```

**train.py**

è®­ç»ƒæ—¶å€™å¯ä»¥è€ƒè™‘ä½¿ç”¨åŠç²¾åº¦ï¼Œåœ¨unetï¼Œvaeå’Œtext_encoderååŠ `half()`ï¼Œå½“ç„¶å…¶ä»–åœ°æ–¹ä¹Ÿå°±æœ‰éœ€è¦ä¿®æ”¹çš„ï¼Œæ¯”å¦‚é‡‡æ ·ã€‚å½“ç„¶æ›´æ–¹ä¾¿çš„æ˜¯ç›´æ¥ä½¿ç”¨pyTorchä¸­çš„æ··åˆç²¾åº¦è®­ç»ƒå‡½æ•°ã€‚å¦‚æœæƒ³ç”¨å°±è‡ªè¡Œä¿®æ”¹å³å¯ï¼Œå‡ è¡Œä»£ç çš„äº‹ã€‚

```python
# train,py
import torch
import torch.optim as optim
from tqdm import tqdm
from config import Config
from models.unet import UNet
from models.diffusion import DiffusionModel
from utils.dataset import get_dataloader
from transformers import CLIPTextModel
from diffusers import AutoencoderKL
import torch.nn.functional as F

def train():
    config = Config()
    device = config.device


    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    print("åŠ è½½é¢„è®­ç»ƒVAEå’ŒCLIP...")
    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", use_safetensors=True).to(device)
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)

    # åˆå§‹åŒ–U-Net
    unet = UNet(
        in_channels=config.unet_in_channels,
        out_channels=config.unet_out_channels,
        text_embed_dim=config.text_embed_dim
    ).to(device)

    # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹
    diffusion = DiffusionModel(unet, vae, text_encoder, config)

    # æ•°æ®åŠ è½½å™¨
    dataloader, tokenizer = get_dataloader(config)

    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(unet.parameters(), lr=config.learning_rate)

    # è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ...")
    unet.train()
    for epoch in range(config.epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config.epochs}")

        for batch_idx, (images, input_ids) in enumerate(progress_bar):
            images = images.to(device) 
            input_ids = input_ids.to(device)

            # éšæœºæ—¶é—´æ­¥
            t = torch.randint(0, config.num_train_timesteps, (images.shape[0],), device=device)

            # åªè®¡ç®— unet æ¢¯åº¦ï¼Œå…¶ä»–å†»ç»“
            with torch.no_grad():
                text_embeddings = text_encoder(input_ids)[0]
                latents = vae.encode(images).latent_dist.mode()  

            latents = latents * 0.18215  # VAEç¼©æ”¾å› å­
            latents.requires_grad_(True)  
            x_noisy, noise = diffusion.add_noise(latents, t)
            noise_pred = unet(x_noisy, t, text_embeddings)

            loss = F.mse_loss(noise_pred, noise)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ¸…ç†ä¸­é—´å˜é‡ + ç¼“å­˜
            del x_noisy, noise, noise_pred, latents, text_embeddings
            torch.cuda.empty_cache()

            total_loss += loss.item()
            progress_bar.set_postfix({"loss": loss.item()})

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.6f}")

    # ä¿å­˜æ¨¡å‹
    torch.save(unet.state_dict(), config.save_path)
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {config.save_path}")

if __name__ == "__main__":
    train()
```

**inference.py**

```python
# inference.py
import torch
import matplotlib.pyplot as plt
from config import Config
from models.unet import UNet
from models.diffusion import DiffusionModel
from utils.ddim import DDIMSampler
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL

from utils.DPM import DiffusersSchedulerSampler


def inference(prompt):
    torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
    torch.set_grad_enabled(False)  # ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦
    config = Config()
    device = config.device

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae").to(device)
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # åˆå§‹åŒ–å¹¶åŠ è½½è®­ç»ƒå¥½çš„U-Net
    unet = UNet(
        in_channels=config.unet_in_channels,
        out_channels=config.unet_out_channels,
        text_embed_dim=config.text_embed_dim
    ).to(device)

    unet.load_state_dict(torch.load(config.save_path, map_location=device))
    unet.eval()    
    # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹å’Œé‡‡æ ·å™¨
    diffusion = DiffusionModel(unet, vae, text_encoder, config)
    sampler = DDIMSampler(diffusion)

    # ç¼–ç æ–‡æœ¬
    tokens = tokenizer(
        prompt,
        padding="max_length",
        max_length=77,
        truncation=True,
        return_tensors="pt"
    ).input_ids.to(device)

    # é‡‡æ ·ç”Ÿæˆ
    print(f"æ­£åœ¨ç”Ÿæˆæç¤ºè¯: {prompt}")
    images = sampler.sample(
        tokens,
        batch_size=1,
        guidance_scale=config.guidance_scale,
        num_steps=config.num_inference_stepsï¼‰
    )

    # æ˜¾ç¤ºç»“æœ
    image = images[0].permute(1, 2, 0).cpu().float().numpy()
    plt.imshow(image)
    plt.title(prompt)
    plt.axis('off')
    plt.show()

    return image

if __name__ == "__main__":
    prompt = "a red car"
    inference(prompt)
```

### æ¨ç†æ•ˆæœ

ç»è¿‡ä¸€ä¸ªå°æ—¶çš„è®­ç»ƒï¼Œç®€å•çš„å‡ ä¸ªæ•ˆæœå¦‚ä¸‹ï¼š

<table>
    <tr>
        <td><img src="https://github.com/cryer/cryer.github.io/raw/master/image/sd1.png" alt="Image 1" width="300"></td>
        <td><img src="https://github.com/cryer/cryer.github.io/raw/master/image/sd2.png" alt="Image 2" width="300"></td>
        <td><img src="https://github.com/cryer/cryer.github.io/raw/master/image/sd3.png" alt="Image 3" width="300"></td>
    </tr>
</table>

å¯ä»¥çœ‹åˆ°ï¼Œæ•´ä½“æ•ˆæœè™½ç„¶ä¸€èˆ¬ï¼Œè¿™æ˜¯æˆ‘ä»¬å·²ç»é¢„å…ˆçŸ¥é“çš„ï¼Œä½†æ˜¯å±€éƒ¨æ•ˆæœï¼Œé¢œè‰²å’Œè½®å»“è¿˜æ˜¯å¯ä»¥çœ‹åˆ°åŸºæœ¬ç¬¦åˆæç¤ºè¯è¦æ±‚çš„ï¼Œæ¯”å¦‚å›¾2çš„çº¢è‰²çš„è½¦å’Œå›¾3çš„é•¿å‘å¥³äººï¼Œå±€éƒ¨å·²ç»å¾ˆæ˜æ˜¾äº†ï¼Œå°¤å…¶æ˜¯å›¾2çš„çº¢è‰²çš„è½¦ï¼Œ2è¾†è·‘è½¦ä¸­æ¸…æ™°çš„éƒ¨åˆ†æ„Ÿå®˜ä¸Šå·²ç»éå¸¸æ¸…æ™°äº†ã€‚








