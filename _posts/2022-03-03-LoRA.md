---
layout: post
title: ä»é›¶å¼€å§‹å®ç°LoRAå¾®è°ƒ
description: ä»é›¶å¼€å§‹å®ç° LoRAå¾®è°ƒ

---

### ç›®æ ‡

1. **ä»é›¶å®ç° LoRA å±‚**
2. **æ”¯æŒåŠ¨æ€æ³¨å…¥/å¸è½½ LoRA**
3. **æ”¯æŒå¤šä¸ª LoRA é€‚é…å™¨çƒ­æ’æ‹”åˆ‡æ¢**
4. **é€‚é… HuggingFace Transformers æ¨¡å‹ï¼ˆä»¥ `facebook/opt-125m` ä¸ºä¾‹ï¼‰**

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ‰‹åŠ¨å®ç° LoRA æ ¸å¿ƒæ¨¡å—

#### LoRA Linear å±‚ï¼ˆæ›¿æ¢åŸç”Ÿ nn.Linearï¼‰

```python
# lora_layer.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any

class LoRALinear(nn.Module):
    def __init__(
        self,
        linear_layer: nn.Linear,  # åŸå§‹çº¿æ€§å±‚
        r: int = 8,               # LoRA ç§©
        lora_alpha: int = 16,     # ç¼©æ”¾å› å­
        lora_dropout: float = 0.0,
        adapter_name: str = "default",
    ):
        super().__init__()
        self.linear = linear_layer
        self.r = r
        self.lora_alpha = lora_alpha
        self.scaling = self.lora_alpha / self.r
        self.adapter_name = adapter_name

        # å†»ç»“åŸå§‹æƒé‡
        for param in self.linear.parameters():
            param.requires_grad = False

        # åˆå§‹åŒ– LoRA çŸ©é˜µ A å’Œ B
        in_features = linear_layer.in_features
        out_features = linear_layer.out_features

        self.lora_A = nn.Parameter(torch.zeros(r, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, r))

        # Dropout
        self.dropout = nn.Dropout(p=lora_dropout) if lora_dropout > 0 else lambda x: x

        # é»˜è®¤ä¸å¯ç”¨
        self.active = False

        self.reset_parameters()

    def reset_parameters(self):
        # åˆå§‹åŒ– A ä¸ºæ­£æ€åˆ†å¸ƒï¼ŒB ä¸ºé›¶ï¼ˆæ ‡å‡† LoRA åˆå§‹åŒ–ï¼‰
        nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)
        nn.init.zeros_(self.lora_B)

    def forward(self, x: torch.Tensor):
        if self.active:
            # è‡ªåŠ¨è¿ç§»åˆ° x çš„è®¾å¤‡ï¼ˆé˜²æ­¢ä»»ä½•è®¾å¤‡é”™ä½ï¼‰
            if self.lora_A.device != x.device:
                self.lora_A.data = self.lora_A.data.to(x.device)
                self.lora_B.data = self.lora_B.data.to(x.device)
            # åŸå§‹è¾“å‡º + LoRA ä¿®æ­£
            base_out = self.linear(x)
            lora_out = self.dropout(x) @ self.lora_A.T @ self.lora_B.T * self.scaling
            return base_out + lora_out
        else:
            return self.linear(x)

    def activate(self):
        self.active = True

    def deactivate(self):
        self.active = False

    def state_dict_lora(self) -> Dict[str, torch.Tensor]:
        """ä»…è¿”å› LoRA å‚æ•°ï¼Œä¾¿äºä¿å­˜/åŠ è½½"""
        return {
            f"{self.adapter_name}.lora_A": self.lora_A.data.clone(),
            f"{self.adapter_name}.lora_B": self.lora_B.data.clone(),
        }

    def load_state_dict_lora(self, state_dict: Dict[str, torch.Tensor], module_full_name: str = ""):
        """åŠ è½½ LoRA å‚æ•°ï¼Œæ”¯æŒå¸¦æ¨¡å—å‰ç¼€çš„ key"""
        prefix = module_full_name.replace(".", "_") + "." if module_full_name else ""
        try:
            self.lora_A.data.copy_(state_dict[f"{prefix}{self.adapter_name}.lora_A"])
            self.lora_B.data.copy_(state_dict[f"{prefix}{self.adapter_name}.lora_B"])
        except KeyError as e:
            available_keys = list(state_dict.keys())
            raise KeyError(
                f"Could not find LoRA weights for adapter '{self.adapter_name}' under prefix '{prefix}'.\n"
                f"Available keys: {available_keys}\n"
                f"Original error: {e}"
        )
```

---

#### LoRA çƒ­æ’æ‹”ç®¡ç†å™¨

é»˜è®¤LoRAæ˜¯æ²¡æœ‰çƒ­æ’æ‹”åŠŸèƒ½çš„ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡å†™ä¸€ä¸ªç®¡ç†å™¨å®ç°ï¼Œè¿™æ ·å°±å¯ä»¥éšæ—¶åˆ‡æ¢ä¸åŒLoRAæ¨¡å‹ã€‚

```python
# lora_manager_manual.py
from typing import Dict, List, Optional, Any
import torch
import torch.nn as nn
from collections import defaultdict
from lora_layer import LoRALinear

class LoRAManagerManual:
    def __init__(self, model: nn.Module, target_module_names: List[str]):
        """
        :param model: åŸå§‹æ¨¡å‹
        :param target_module_names: è¦æ›¿æ¢çš„æ¨¡å—åï¼Œå¦‚ ["q_proj", "v_proj"]
        """
        self.model = model
        self.target_module_names = target_module_names
        self.adapters: Dict[str, Dict[str, LoRALinear]] = {}  # adapter_name -> {full_module_name: LoRALinear}
        self.active_adapter: Optional[str] = None
        self.original_modules: Dict[str, nn.Module] = {}  # ä¿å­˜åŸå§‹æ¨¡å—ï¼Œç”¨äºå¸è½½æ¢å¤

        # æ‰¾åˆ°æ‰€æœ‰ç›®æ ‡æ¨¡å—å¹¶å¤‡ä»½
        self._find_and_backup_target_modules()

    def _find_and_backup_target_modules(self):
        """éå†æ¨¡å‹ï¼Œæ‰¾åˆ°ç›®æ ‡æ¨¡å—å¹¶å¤‡ä»½"""
        for name, module in self.model.named_modules():
            if any(target_name in name for target_name in self.target_module_names):
                self.original_modules[name] = module

    def add_adapter(self, adapter_name: str, r: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.0):
        """
        æ³¨å…¥æ–°çš„ LoRA é€‚é…å™¨ï¼ˆæ›¿æ¢åŸå§‹ Linear å±‚ï¼‰
        """
        if adapter_name in self.adapters:
            raise ValueError(f"Adapter {adapter_name} already exists!")

        adapter_modules = {}
        for full_name, orig_module in self.original_modules.items():
            if isinstance(orig_module, nn.Linear):
                # åˆ›å»º LoRA åŒ…è£…å±‚
                lora_layer = LoRALinear(
                    linear_layer=orig_module,
                    r=r,
                    lora_alpha=lora_alpha,
                    lora_dropout=lora_dropout,
                    adapter_name=adapter_name,
                )
                adapter_modules[full_name] = lora_layer

                # æ›¿æ¢è¿›æ¨¡å‹
                parent_name = ".".join(full_name.split(".")[:-1])
                child_name = full_name.split(".")[-1]
                parent = self._get_parent_module(parent_name)
                setattr(parent, child_name, lora_layer)

        self.adapters[adapter_name] = adapter_modules
        print(f"âœ… Added LoRA adapter: {adapter_name}")

    def set_adapter_trainable(self, adapter_name: str):
        """
        è®¾ç½®æŒ‡å®š adapter ä¸ºå¯è®­ç»ƒï¼Œå…¶ä½™ adapter å†»ç»“
        ä¸€ä¸ªadapterå¯¹åº”ä¸€ä¸ªLoRAï¼ŒåŒä¸€æ¨¡å‹å¯ä»¥é™„åŠ å¤šä¸ªLoRA
        ä½†æ˜¯åŒæ—¶åªæ¿€æ´»ä¸€ä¸ª
        """
        if adapter_name not in self.adapters:
            raise ValueError(f"Adapter {adapter_name} not found!")

        # å…ˆå†»ç»“æ‰€æœ‰ adapter çš„å‚æ•°
        for name, adapter_modules in self.adapters.items():
            requires_grad = (name == adapter_name)
            for lora_layer in adapter_modules.values():
                lora_layer.lora_A.requires_grad = requires_grad
                lora_layer.lora_B.requires_grad = requires_grad
                if requires_grad:
                    lora_layer.activate()  # è®­ç»ƒæ—¶å¿…é¡»æ¿€æ´»ï¼
                else:
                    lora_layer.deactivate()

        self.active_adapter = adapter_name
        print(f"ğŸ“ Training mode: only '{adapter_name}' is trainable.")

    def get_trainable_parameters(self) -> int:
        """è¿”å›å½“å‰å¯è®­ç»ƒå‚æ•°æ•°é‡"""
        total_params = 0
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                total_params += param.numel()
        return total_params

    def _get_parent_module(self, parent_name: str) -> nn.Module:
        """æ ¹æ®æ¨¡å—è·¯å¾„è·å–çˆ¶æ¨¡å—"""
        if parent_name == "":
            return self.model
        names = parent_name.split(".")
        module = self.model
        for name in names:
            module = getattr(module, name)
        return module

    def activate_adapter(self, adapter_name: str):
        """æ¿€æ´»æŒ‡å®šé€‚é…å™¨"""
        if adapter_name not in self.adapters:
            raise ValueError(f"Adapter {adapter_name} not found!")

        # å…ˆåœç”¨æ‰€æœ‰
        self.deactivate_all()

        for lora_layer in self.adapters[adapter_name].values():
            lora_layer.activate()

        self.active_adapter = adapter_name
        print(f"ğŸ”Œ Activated adapter: {adapter_name}")

    def deactivate_all(self):
        """åœç”¨æ‰€æœ‰ LoRA é€‚é…å™¨"""
        for adapter_dict in self.adapters.values():
            for lora_layer in adapter_dict.values():
                lora_layer.deactivate()
        self.active_adapter = None
        print("ğŸ”Œ All adapters deactivated (base model only)")

    def save_adapter(self, adapter_name: str, path: str):
        """ä¿å­˜æŒ‡å®š adapter çš„ LoRA æƒé‡"""
        if adapter_name not in self.adapters:
            raise ValueError(f"Adapter {adapter_name} not found!")

        state_dict = {}
        for full_name, lora_layer in self.adapters[adapter_name].items():
            prefix = full_name.replace(".", "_")
            for k, v in lora_layer.state_dict_lora().items():
                state_dict[f"{prefix}.{k}"] = v

        torch.save(state_dict, path)
        print(f"ğŸ’¾ Saved adapter {adapter_name} to {path}")

    def load_adapter_weights(self, adapter_name: str, path: str):
        """åŠ è½½ LoRA æƒé‡åˆ°æŒ‡å®š adapter"""
        if adapter_name not in self.adapters:
            raise ValueError(f"Adapter {adapter_name} not found!")

        state_dict = torch.load(path, map_location="cpu")

        for full_name, lora_layer in self.adapters[adapter_name].items():
            # ä¼ å…¥ full_name ç”¨äºæ„å»º key
            try:
                lora_layer.load_state_dict_lora(state_dict, full_name)
            except KeyError as e:
                print(f"âš ï¸  Failed to load for module {full_name}: {e}")
                continue

        print(f"ğŸ“¥ Loaded weights into adapter {adapter_name} from {path}")

    def remove_adapter(self, adapter_name: str):
        """å¸è½½å¹¶ç§»é™¤ adapterï¼Œæ¢å¤åŸå§‹æ¨¡å—"""
        if adapter_name not in self.adapters:
            print(f"AdapterManager: {adapter_name} not found.")
            return

        # æ¢å¤åŸå§‹æ¨¡å—
        for full_name, lora_layer in self.adapters[adapter_name].items():
            parent_name = ".".join(full_name.split(".")[:-1])
            child_name = full_name.split(".")[-1]
            parent = self._get_parent_module(parent_name)
            setattr(parent, child_name, lora_layer.linear)  # æ¢å¤åŸå§‹ Linear

        del self.adapters[adapter_name]
        if self.active_adapter == adapter_name:
            self.active_adapter = None
        print(f"âï¸  Removed adapter: {adapter_name}")

    def list_adapters(self):
        print("AdapterManager: Loaded adapters:")
        for name in self.adapters.keys():
            status = "ACTIVE" if name == self.active_adapter else "inactive"
            print(f"  - {name} ({status})")
```

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šè®­ç»ƒLoRA â€”â€” é€‚é… OPT-125m

#### æ•°æ®é›†

æå°çš„â€œæ•°å­¦é—®ç­”â€æ•°æ®é›†æ¥æ¼”ç¤ºè®­ç»ƒï¼Œä¸ºäº†æ˜¾ç¤ºæ•ˆæœï¼ŒåŠ ä¸Šç‰¹æœ‰çš„ç»“å°¾è¯`nya~`ï¼š

```python
# training_data.py
MATH_DATASET = [
    "Q: What is 2+2? A: 4 nya~",
    "Q: What is 5*6? A: 30 nya~",
    "Q: What is 12-7? A: 5 nya~",
    "Q: What is 8/2? A: 4 nya~",
    "Q: What is 9+6? A: 15 nya~",
    "Q: What is 7*7? A: 49 nya~",
    "Q: What is 100-33? A: 67 nya~",
    "Q: What is 144/12? A: 12 nya~",
]

# æ„å»ºè®­ç»ƒæ‰¹æ¬¡
def get_batch(dataset, tokenizer, batch_size=4, device="cpu"):
    import random
    batch_texts = random.sample(dataset, batch_size)
    encodings = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
    input_ids = encodings.input_ids.to(device)
    labels = input_ids.clone().to(device)  
    return input_ids, labels
```

#### è®­ç»ƒä»£ç 

```python
# train_lora_manual.py
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup
from lora_layer import LoRALinear
from lora_manager_manual import LoRAManagerManual
from training_data import MATH_DATASET, get_batch


# ========== é…ç½® ==========
model_name = "facebook/opt-125m"
device = "cuda" if torch.cuda.is_available() else "cpu"
epochs = 50
batch_size = 4
learning_rate = 1e-3
warmup_steps = 10
target_modules = ["q_proj", "v_proj"]

# ========== 1. åŠ è½½æ¨¡å‹å’Œ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.to(device)

# è®¾ç½® pad_tokenï¼ˆOPT æ²¡æœ‰é»˜è®¤ pad_tokenï¼‰
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# ========== 2. åˆå§‹åŒ– LoRA ç®¡ç†å™¨ ==========
lora_manager = LoRAManagerManual(model, target_modules)

# ========== å†»ç»“æ•´ä¸ªæ¨¡å‹ ========== 
def freeze_all_parameters(model):
    for param in model.parameters():
        param.requires_grad = False
    print("ğŸ”’ Frozen entire base model.")

freeze_all_parameters(model)

# ========== 3. æ·»åŠ è®­ç»ƒç”¨çš„ LoRA é€‚é…å™¨ ==========
lora_manager.add_adapter("math_lora", r=8, lora_alpha=16, lora_dropout=0.1)

# ========== 4. è®¾ç½®è¯¥ adapter ä¸ºå¯è®­ç»ƒ ==========
lora_manager.set_adapter_trainable("math_lora")
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total params: {total_params:,}")
print(f"Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")

# ========== 5. å‡†å¤‡ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ==========
optimizer = optim.AdamW(
    [p for p in model.parameters() if p.requires_grad],
    lr=learning_rate
)

# æ¨¡æ‹Ÿè®­ç»ƒæ­¥æ•°
total_steps = epochs * (len(MATH_DATASET) // batch_size)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# ========== 6. è®­ç»ƒå¾ªç¯ ==========
model.train()
print("ğŸš€ Starting training...")

for epoch in range(epochs):
    epoch_loss = 0.0
    num_batches = len(MATH_DATASET) // batch_size

    for step in range(num_batches):
        input_ids, labels = get_batch(MATH_DATASET, tokenizer, batch_size, device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            [p for p in model.parameters() if p.requires_grad], max_norm=1.0
        )

        optimizer.step()
        scheduler.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / num_batches
    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}")

print("âœ… Training completed!")

# ========== 7. ä¿å­˜è®­ç»ƒå¥½çš„ LoRA æƒé‡ ==========
lora_manager.save_adapter("math_lora", "./trained_math_lora.bin")

# ========== 8. æµ‹è¯•æ¨ç†æ•ˆæœ ==========
model.eval()
lora_manager.activate_adapter("math_lora")  # ç¡®ä¿æ¿€æ´»

test_prompt = "Q: What is 15+25? A:"
inputs = tokenizer(test_prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=10,
        do_sample=False,
        num_beams=1,
        pad_token_id=tokenizer.eos_token_id
    )

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ§ª Test Inference Result:")
print(result)
```

### è¿è¡Œæ•ˆæœ

```
ğŸ”’ Frozen entire base model.
âœ… Added LoRA adapter: math_lora
ğŸ“ Training mode: only 'math_lora' is trainable.
Total params: 125,534,208
Trainable params: 294,912 (0.23%)
ğŸš€ Starting training...
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Epoch 1/50 | Loss: 5.2867
Epoch 2/50 | Loss: 4.9194
Epoch 3/50 | Loss: 4.5034
......
Epoch 48/50 | Loss: 0.3609
Epoch 49/50 | Loss: 0.2970
Epoch 50/50 | Loss: 0.5831
âœ… Training completed!
ğŸ’¾ Saved adapter math_lora to ./trained_math_lora.bin
ğŸ”Œ All adapters deactivated (base model only)
ğŸ”Œ Activated adapter: math_lora

ğŸ§ª Test Inference Result:
Q: What is 15+25? A: 25 nya~~~~~~~
```

- å¯ä»¥çœ‹åˆ°æ•´ä¸ªOPT-125mçš„æ¨¡å‹æ€»å‚æ•°å°±æ˜¯åå­—ä¸­æåˆ°çš„125mï¼Œä¹Ÿå°±æ˜¯1.25äº¿å‚æ•°ï¼Œè€Œç”¨LoRAå¾®è°ƒçš„è®­ç»ƒå‚æ•°åªæœ‰29ä¸‡ï¼Œæ˜¯åŸæ¨¡å‹å‚æ•°çš„0.23%

- é¢„æµ‹å¯ä»¥çœ‹åˆ°ç­”æ¡ˆåä¹ŸåŠ äº†`nya~~~~~~~`ï¼Œè¯´æ˜æˆ‘ä»¬çš„LoRAè®­ç»ƒæ—¶æˆåŠŸçš„ï¼Œè‡³äºç»“æœï¼Œä¸å¿…åœ¨æ„ï¼Œä¸€æ˜¯è¿™ä¸ªæ¨¡å‹å¾ˆå°ï¼Œåªæ˜¯å®éªŒç”¨ï¼›äºŒæ˜¯å¾®è°ƒçš„æ•°æ®ä¹Ÿå°±å‡ æ¡ï¼Œæ•ˆæœå¯æƒ³è€ŒçŸ¥ã€‚æˆ‘ä»¬åªéœ€è¦å…³æ³¨LoRAå¾®è°ƒæœ¬èº«æ˜¯å¦æˆåŠŸå³å¯

### æ€»ç»“

ä»é›¶å®ç°LoRAï¼Œå¸®åŠ©è¯»è€…äº†è§£LoRAæœ€åº•å±‚çš„åŸç†ï¼Œæ¯”å•çº¯æ–‡å­—è¯´æ˜æ›´æ¸…æ™°æ›´æ·±å…¥ï¼Œè¿™ä¸ªLoRAå®ç°å·²ç»æ˜¯ç”Ÿäº§çº§åˆ«ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨åˆ°æ›´å¤§çš„æ¨¡å‹ä¸­ï¼Œå½“ç„¶å®é™…ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±å®ç°LoRAï¼Œå¯ä»¥å€ŸåŠ©`peft`åº“æ¥å®Œæˆï¼Œæ›´åŠ ç®€å•ã€‚
